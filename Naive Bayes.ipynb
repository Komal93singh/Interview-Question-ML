{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d3005",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25100b00",
   "metadata": {},
   "source": [
    "# QUIZ : NAIVE BAYES\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94da5ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52eb4f22",
   "metadata": {},
   "source": [
    "## Q1. Which type of Naive Bayes is best suited for text classification with word count features? \n",
    "1. Gaussian Naive Bayes \n",
    "2. Bernoulli Naive Bayes \n",
    "3. Multinomial Naive Bayes \n",
    "4. Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067863f4",
   "metadata": {},
   "source": [
    "The best-suited type of Naive Bayes for text classification with **word count features** is:\n",
    "\n",
    "**3. Multinomial Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **Multinomial Naive Bayes** works well with discrete features such as word counts or frequencies, which makes it ideal for text classification tasks using bag-of-words or TF-IDF features.\n",
    "* **Bernoulli Naive Bayes** is suitable for binary/boolean features (word presence/absence).\n",
    "* **Gaussian Naive Bayes** assumes continuous features with a normal distribution, so it is less suitable for text data.\n",
    "* **Complement Naive Bayes** is a variant designed to handle imbalanced datasets but is still based on multinomial distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b53ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a9a0bb",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using scikit-learn's implementation of Naive Bayes over a custom implementation? \n",
    "1. It's always more accurate \n",
    "2. It's optimized for performance \n",
    "3. It can handle any type of data \n",
    "4. It doesn't require data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80ede3",
   "metadata": {},
   "source": [
    "The main advantage of using scikit-learn’s implementation of Naive Bayes over a custom implementation is:\n",
    "\n",
    "**2. It’s optimized for performance**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Scikit-learn’s Naive Bayes implementations are highly optimized, efficient, and tested for performance and scalability.\n",
    "* It’s not necessarily *always more accurate* (option 1), since accuracy depends on the data and model tuning.\n",
    "* It **cannot handle any type of data** automatically (option 3); data preprocessing is often still required.\n",
    "* It **does require data preprocessing** like feature extraction and cleaning (option 4 is incorrect).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4f027",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b948c2cb",
   "metadata": {},
   "source": [
    "## Q3. What does the 'np.unique' function do in the context of Naive Bayes implementation? \n",
    "1. It nomralizes the feature values \n",
    "2. It calaculates the mean of features \n",
    "3. It finds the unique classes or feature values \n",
    "4. It splits the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb4d8b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It finds the unique classes or feature values**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* `np.unique` returns the sorted unique elements of an array.\n",
    "* In Naive Bayes implementation, it's often used to find the unique class labels or unique feature values.\n",
    "* It does **not** normalize features (option 1), calculate means (option 2), or split datasets (option 4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e5964",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2707d3c4",
   "metadata": {},
   "source": [
    "## Q4. What is the purpose of the '_predict_sample' method in th ecustom Naive Bayes implementation? \n",
    "1. To train the model on a single sample \n",
    "2. To calculate the probability for a single instance \n",
    "3. To evaluate the model's performance \n",
    "4. To preprocess a single data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459db47a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To calculate the probability for a single instance**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* In a custom Naive Bayes implementation, the `_predict_sample` method typically computes the predicted class probabilities (or the most likely class) for **one input sample** using the learned probabilities from training.\n",
    "* It is **not** for training (option 1), evaluating performance (option 3), or preprocessing (option 4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3bcfe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d822238a",
   "metadata": {},
   "source": [
    "## Q5. What does the 'complementNB' class in scikit-learn implement? \n",
    "1. A variant of Gaussian Naive Bayes \n",
    "2. A variant of Multinomial Naive Bayes for imbalanced datasets \n",
    "3. A combination of Multinomial and Bernoulli Naive Bayes \n",
    "4. A Naive Bayes classifier for continuous features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be90d7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. A variant of Multinomial Naive Bayes for imbalanced datasets**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The `ComplementNB` class in scikit-learn is designed to improve the performance of Multinomial Naive Bayes, especially on **imbalanced datasets**.\n",
    "* It adjusts the weight of features based on the complement of each class.\n",
    "* It is **not** a variant of Gaussian Naive Bayes (option 1), nor a combination of Multinomial and Bernoulli (option 3), nor meant for continuous features (option 4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6ded5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de392f1",
   "metadata": {},
   "source": [
    "## Q6. In the context of Naive Bayes, what is the 'prior probability'? \n",
    "1. The probability of a feature given a class \n",
    "2. The probability of a class before seeing any features \n",
    "3. The final probability after classification \n",
    "4. The probability of a feature occurring in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a229e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The probability of a class before seeing any features**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The **prior probability** in Naive Bayes refers to the initial probability of each class based on the training data, *before* considering any feature values.\n",
    "* Option 1 describes the likelihood $P(\\text{feature}|\\text{class})$.\n",
    "* Option 3 is the posterior probability after applying Bayes' theorem.\n",
    "* Option 4 is unrelated to the concept of prior probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc09a72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26741882",
   "metadata": {},
   "source": [
    "## Q7. Which Naive Bayes variant would be most appropriate for classoifying emails based on th efrequency of certain words? \n",
    "1. Gaussian Naive Bayes \n",
    "2. Bernoulli naive Bayes \n",
    "3. Multinomial Naive Bayes \n",
    "4. Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8741f",
   "metadata": {},
   "source": [
    "The best choice is:\n",
    "\n",
    "**3. Multinomial Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Why?\n",
    "\n",
    "* For classifying emails based on **word frequency counts**, Multinomial Naive Bayes is ideal because it models discrete count data.\n",
    "* **Bernoulli Naive Bayes** works on binary features (presence/absence of words), not frequencies.\n",
    "* **Gaussian Naive Bayes** assumes continuous, normally distributed features, which doesn't fit word counts.\n",
    "* **Complement Naive Bayes** is a variant of Multinomial NB, useful especially for imbalanced data, but Multinomial NB is the primary go-to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7f0b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81a501f",
   "metadata": {},
   "source": [
    "## Q8. In Multinomial Naive Bayes, what do the features typically represent? \n",
    "1. Continuous measurements \n",
    "2. Binary indicators \n",
    "3. Word frequencies \n",
    "4. Image pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f19cf",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Word frequencies**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* In **Multinomial Naive Bayes**, features usually represent **counts or frequencies of words** (or tokens) in text classification.\n",
    "* Continuous measurements (option 1) are more suited for Gaussian NB.\n",
    "* Binary indicators (option 2) are typical for Bernoulli NB.\n",
    "* Image pixels (option 4) are not typical features for Multinomial NB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce8946",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1615d81",
   "metadata": {},
   "source": [
    "## Q9. What assumption does Gaussian Naive Bayes make about the features? \n",
    "1. They follow a Bernoulli distribution \n",
    "2. They follow a Gaussian distribution \n",
    "3. They are binary \n",
    "4. They are discrete counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155ff8f",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. They follow a Gaussian distribution**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Gaussian Naive Bayes assumes that the features are **continuous and normally (Gaussian) distributed** within each class.\n",
    "* Bernoulli distribution (option 1) applies to Bernoulli NB.\n",
    "* Binary features (option 3) are for Bernoulli NB.\n",
    "* Discrete counts (option 4) are modeled by Multinomial NB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d1846",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "367ca6da",
   "metadata": {},
   "source": [
    "## Q10. Which Naive Bayes variant is designed for binary/Boolean features? \n",
    "1. Multinomial Naive Bayes \n",
    "2. Gaussian Naive Bayes \n",
    "3. Bernoulli Naive Bayes \n",
    "4. Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c481246",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Bernoulli Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Bernoulli Naive Bayes is specifically designed for **binary/Boolean features** (e.g., word presence or absence).\n",
    "* Multinomial NB (option 1) is for count/frequency features.\n",
    "* Gaussian NB (option 2) is for continuous features.\n",
    "* Complement NB (option 4) is a variant of Multinomial NB, mainly for imbalanced data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff26f85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b19a4ae",
   "metadata": {},
   "source": [
    "## Q11. What is the primary purpose of complement Naive Bayes? \n",
    "1. To handle continuous features \n",
    "2. To work with imbalanced datasets \n",
    "3. To process binary features \n",
    "4. To improve accuracy on balanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb684a4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To work with imbalanced datasets**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Complement Naive Bayes was developed to improve performance specifically on **imbalanced datasets** by adjusting how feature weights are calculated using the complement of each class.\n",
    "* It is **not** designed for continuous features (option 1), binary features (option 3), or balanced datasets (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178680f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e664f1",
   "metadata": {},
   "source": [
    "## Q12. In scikit-learn, which class is used to implement Multinomial Naive Bayes? \n",
    "1. GaussianNB \n",
    "2. BernoulliNB \n",
    "3. MultinomialNB \n",
    "4. ComplementNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43cf57",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. MultinomialNB**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* In scikit-learn, the **MultinomialNB** class implements the Multinomial Naive Bayes algorithm.\n",
    "* **GaussianNB** is for Gaussian Naive Bayes.\n",
    "* **BernoulliNB** is for Bernoulli Naive Bayes.\n",
    "* **ComplementNB** is for Complement Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e474aa6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d43ce9",
   "metadata": {},
   "source": [
    "## Q13. What preprocessing step is typically requires before using Multinomial Naive Bayes for text classification? \n",
    "1. Normalization \n",
    "2. Vectorization \n",
    "3. Standardization \n",
    "4. Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4603e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Vectorization**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Before using Multinomial Naive Bayes for text classification, you typically need to **convert text data into numerical features** using **vectorization** methods such as **CountVectorizer** or **TF-IDF Vectorizer**.\n",
    "* Normalization (option 1) and standardization (option 3) are more common for continuous features, not counts.\n",
    "* Binarization (option 4) is more relevant for Bernoulli Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4900a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa99f87",
   "metadata": {},
   "source": [
    "## Q14. Which Naive Bayes variant would you use for classifying data points based on their coordinates ? \n",
    "1. Multinomial Naive Bayes \n",
    "2. Gaussian Naive Bayes \n",
    "3. Bernoulli Naive Bayes \n",
    "4. Complement NAive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f44ca1",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Gaussian Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* When classifying data points based on **continuous features** like coordinates (e.g., x and y values), **Gaussian Naive Bayes** is appropriate because it assumes features follow a Gaussian (normal) distribution.\n",
    "* Multinomial and Complement Naive Bayes are for discrete/count data.\n",
    "* Bernoulli Naive Bayes is for binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacc21a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462c3024",
   "metadata": {},
   "source": [
    "## Q15. What is Laplace smoothing used for in Naive Bayes? \n",
    "1. To normalize features \n",
    "2. To handle missing data \n",
    "3. To prevent zero probabilities \n",
    "4. To improve computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb7602",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. To prevent zero probabilities**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **Laplace smoothing** (also called add-one smoothing) is used in Naive Bayes to avoid zero probabilities for features that don’t appear in the training data for a given class.\n",
    "* It adds a small constant (usually 1) to feature counts to ensure every feature has a non-zero probability.\n",
    "* It does **not** normalize features (option 1), handle missing data (option 2), or directly improve computational efficiency (option 4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b955394",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9ab026",
   "metadata": {},
   "source": [
    "## Q16. Which Naive Bayes variant is most suitable for sentiment analysis of short text messages? \n",
    "1. Gaussian Naive Bayes \n",
    "2. Bernoulli Naive Bayes \n",
    "3. Multinomial NAive Bayes \n",
    "4. Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179664",
   "metadata": {},
   "source": [
    "The best choice is:\n",
    "\n",
    "**3. Multinomial Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* For **sentiment analysis of short text messages**, which usually involves word counts or frequencies, Multinomial Naive Bayes is the most suitable.\n",
    "* Bernoulli NB (option 2) can be used but works better with binary presence/absence features.\n",
    "* Gaussian NB (option 1) is for continuous data, so less appropriate here.\n",
    "* Complement NB (option 4) is useful for imbalanced datasets but is still based on the multinomial model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247e4f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "873a3b68",
   "metadata": {},
   "source": [
    "## Q17. WHat does the CountVectorizer in scikit-learn do? \n",
    "1. Normalizes the input data \n",
    "2. Converts text to a matrix of token counts \n",
    "3. Applies Gaussian distribution to features \n",
    "4. Binarizes the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f5584",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Converts text to a matrix of token counts**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **CountVectorizer** in scikit-learn converts a collection of text documents into a matrix of **token counts** (i.e., how many times each word appears).\n",
    "* It does **not** normalize data (option 1), apply Gaussian distribution (option 3), or binarize features (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede3e35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade7fc01",
   "metadata": {},
   "source": [
    "## Q18. IN the custom NAive Bayes implementation, what does the line 'self.feature_probs[c][feature, value] = (np.sum(X_c[:,feature] == value) + 1)/feature_sum' calculate? \n",
    "1. Prior probability \n",
    "2. Likelihood \n",
    "3. Posterior probability \n",
    "4. Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47efa1",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Likelihood**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* This line calculates the **likelihood** $P(\\text{feature} = \\text{value} | \\text{class} = c)$, i.e., the probability of a specific feature value given a class.\n",
    "* The `+1` is Laplace smoothing to avoid zero probabilities.\n",
    "* Prior probability (option 1) refers to $P(\\text{class} = c)$.\n",
    "* Posterior probability (option 3) is computed after combining prior and likelihood.\n",
    "* Evidence (option 4) is the overall probability of the data, used for normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ff460",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "020d08df",
   "metadata": {},
   "source": [
    "## Q19. What is the purpose of using 'np.log' in the custom Naive Bayes implementation ? \n",
    "1. To normalize probabilities \n",
    "2. To prevent underflow \n",
    "3. To speed up calaculations \n",
    "4. To handle negative values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8af90d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To prevent underflow**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Taking the **logarithm (np.log)** of probabilities helps prevent numerical **underflow**, which occurs when multiplying many small probabilities together results in values too close to zero for the computer to represent.\n",
    "* Logarithms convert multiplication into addition, which is numerically more stable.\n",
    "* It doesn’t normalize probabilities (option 1), directly speed up calculations (option 3), or handle negative values (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2024d32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27fbd7dc",
   "metadata": {},
   "source": [
    "## Q20. Which Naive BAyes variant would be most appropriate for spam detection based on the presence or absence of certain words? \n",
    "1. Gaussian Naive Bayes \n",
    "2. Bernoulli NAive Bayes \n",
    "3. Multinomial Naive BAyes \n",
    "4. COmplement Naive BAyes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3700d9",
   "metadata": {},
   "source": [
    "The best choice is:\n",
    "\n",
    "**2. Bernoulli Naive Bayes**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Bernoulli Naive Bayes works well when features are **binary indicators** — i.e., presence or absence of words — which fits spam detection based on whether certain words appear or not.\n",
    "* Multinomial NB (option 3) models counts/frequencies, not just presence.\n",
    "* Gaussian NB (option 1) is for continuous features.\n",
    "* Complement NB (option 4) is a variant of Multinomial NB, mainly for imbalanced data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49691e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ee9bfd",
   "metadata": {},
   "source": [
    "## Q21. What does the 'fit' method do in scikit-learn's Naive Bayes implemenations? \n",
    "1. Makes predictions \n",
    "2. Evaluates the model \n",
    "3. Trains the model \n",
    "4. Preprocesses the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb402c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Trains the model**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The `fit` method in scikit-learn is used to **train the model** on the given data by learning the parameters (like probabilities in Naive Bayes).\n",
    "* It does **not** make predictions (option 1), evaluate the model (option 2), or preprocess data (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f951e53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb55012a",
   "metadata": {},
   "source": [
    "## Q22. In the context of Naive Bayes, what does the term 'naive' refer to? \n",
    "1. The simplicity of the algorithm \n",
    "2. The assumption of feature independence \n",
    "3. The use of prior probabilities \n",
    "4. The speed of the slgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917f554",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The assumption of feature independence**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The term **\"naive\"** in Naive Bayes refers to the **strong assumption that all features are independent of each other given the class label**.\n",
    "* This assumption is often not true in real-world data, but it simplifies computation and often works well in practice.\n",
    "* It’s not just about simplicity (option 1), use of priors (option 3), or speed (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd57cec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f05e84e",
   "metadata": {},
   "source": [
    "## Q23. Which of the following is NOT a common application of Naive Bayes? \n",
    "1. Text Calssification \n",
    "2. Spam filtering \n",
    "3. Sentiment analysis \n",
    "4. Image segematation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7939e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Image segmentation**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Naive Bayes is commonly used for text classification, spam filtering, and sentiment analysis — all tasks involving categorical or text data.\n",
    "* Image segmentation (option 4) usually requires more complex models like convolutional neural networks (CNNs), not Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb627",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c0913a",
   "metadata": {},
   "source": [
    "## Q24. What is the amin advantage of Naive Bayes classifiers? \n",
    "1. They always provide the highest accuracy \n",
    "2. They can handle non-linear relationships \n",
    "3. They are simple and fast \n",
    "4. They don't require feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405cb2f5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. They are simple and fast**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Naive Bayes classifiers are **simple to implement and computationally efficient**, making them fast especially for large datasets.\n",
    "* They do **not** always provide the highest accuracy (option 1).\n",
    "* They assume feature independence, so they generally don’t handle complex non-linear relationships well (option 2).\n",
    "* While they often don’t require feature scaling (option 4), the main advantage is simplicity and speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7119cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f424b2",
   "metadata": {},
   "source": [
    "## Q25. What is the purpose of the 'predict' method in Naive Bayes implemenattions? \n",
    "1. To train the model \n",
    "2. To calacualte probabilities \n",
    "3. To evaluate the model \n",
    "4. To classify new instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489e95a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. To classify new instances**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The `predict` method in Naive Bayes is used to **classify new, unseen data points** based on the learned model.\n",
    "* It does **not** train the model (option 1), calculate probabilities explicitly for each feature (option 2, though it uses probabilities internally), or evaluate the model (option 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be93ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "637e71f2",
   "metadata": {},
   "source": [
    "## Q26. What does the 'random_state' parameter in 'train_test_split' control? \n",
    "1. The randomness of the Naive Bayes algorithm \n",
    "2. The seed for random number generation in splitting the data \n",
    "3. The number of features to consider \n",
    "4. The balance of classes in the split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277c6e4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. The seed for random number generation in splitting the data**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The `random_state` parameter in `train_test_split` sets the **seed for the random number generator** to ensure reproducible splits of data into training and testing sets.\n",
    "* It does **not** control the randomness of the Naive Bayes algorithm (option 1), the number of features (option 3), or the class balance (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaa9e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa09f995",
   "metadata": {},
   "source": [
    "## Q27. WHat is the main difference between Multinomial and Bernoulli Naive Bayes? \n",
    "1. Multinomial is faster \n",
    "2. Bernoulli handles continous features \n",
    "3. Mutlinomial uses word frequencies, Bernoulli uses word presence \n",
    "4. Bernoulli is more accurate for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edcb59",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Multinomial uses word frequencies, Bernoulli uses word presence**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* **Multinomial Naive Bayes** models **counts or frequencies** of features (e.g., how many times a word appears).\n",
    "* **Bernoulli Naive Bayes** models **binary features** representing **presence or absence** of a feature.\n",
    "* Speed (option 1) and accuracy (option 4) depend on the dataset and are not absolute.\n",
    "* Bernoulli NB does **not** handle continuous features (option 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56743041",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dede3917",
   "metadata": {},
   "source": [
    "## Q28. In the custom Naive Bayes Implemnation, what does 'defaultdict(lambda: defaultdict(lambda: 1))' achieve? \n",
    "1. It sets all probabilities to 1 \n",
    "2. It implements Laplace smoothing \n",
    "3. It creates a nested dictionary with default value 1 \n",
    "4. It normalizes the probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b026f3",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It creates a nested dictionary with default value 1**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* `defaultdict(lambda: defaultdict(lambda: 1))` creates a **nested dictionary** where if a key is missing at any level, it automatically returns `1` instead of raising a KeyError.\n",
    "* This is often used to simplify counting and **implement Laplace smoothing** by starting counts at 1 (related to option 2, but the actual smoothing happens in the calculation).\n",
    "* It does **not** set all probabilities to 1 (option 1) or normalize probabilities (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed4525",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d798e6",
   "metadata": {},
   "source": [
    "## Q29. Which of the following is true about the 'GaussianNB' class in scikit-learn? \n",
    "1. It's suitable for text classification \n",
    "2. It assumes features follow a Gaussian distribution \n",
    "3. It's best for binary features \n",
    "4. It's designed for imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5027343",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It assumes features follow a Gaussian distribution**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* The **GaussianNB** class assumes that features are continuous and **normally (Gaussian) distributed** within each class.\n",
    "* It is **not** ideal for text classification (option 1), which typically involves discrete features.\n",
    "* It’s **not** designed for binary features (option 3) — that’s BernoulliNB.\n",
    "* It is **not** specifically designed for imbalanced datasets (option 4).\n",
    "\n",
    "Want me to explain when to use GaussianNB versus other Naive Bayes variants?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213f755",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ddd8a2",
   "metadata": {},
   "source": [
    "## Q30. In the custom Naive Bayes implementation, why is 'np.log' used instead of direct multiplication of probabilities? \n",
    "1. To speed up calculations \n",
    "2. To normalize probabilities \n",
    "3. To avoid underflow errors \n",
    "4. To implement Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308dac0",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. To avoid underflow errors**\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* Multiplying many small probabilities can result in values too close to zero for the computer to represent (underflow).\n",
    "* Using `np.log` transforms multiplication into addition, making calculations numerically stable.\n",
    "* It’s not primarily for speeding up calculations (option 1), normalizing probabilities (option 2), or Laplace smoothing (option 4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0e5b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af394",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ed5e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc9822",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
