{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6da8b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a954f3a",
   "metadata": {},
   "source": [
    "# QUIZ : LOGISTICS REGRESSION \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7dc2e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc7b5de6",
   "metadata": {},
   "source": [
    "## Q1. Which metric is best suited for imbalanced classification problems? \n",
    "1. Accuracy \n",
    "2. Precision \n",
    "3. Recall \n",
    "4. F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070f295",
   "metadata": {},
   "source": [
    "For **imbalanced classification problems**, **Recall** (option 3) is often the best suited metric — especially when the goal is to minimize false negatives (e.g., in medical diagnosis, fraud detection).\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "* **Accuracy** can be misleading because the majority class dominates.\n",
    "* **Precision** only focuses on false positives, which might not be enough if missing a positive case is costly.\n",
    "* **Recall** captures the ability to detect most of the actual positives, which is usually the priority in imbalanced datasets.\n",
    "* **F1 Score** is good if you need a balance between precision and recall, but if *catching as many positives as possible* is your priority, recall is the main metric.\n",
    "\n",
    "If you want the **most balanced single metric** for imbalanced data, you’d go with **F1 score**, but if the exam is asking **the best suited when missing positives is costly**, the correct answer is **Recall**.\n",
    "\n",
    "I can explain when **F1** would beat **Recall** if you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162bc7e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd36718",
   "metadata": {},
   "source": [
    "## Q2. Which of the following is NOT an application of Decision Tree Regression? \n",
    "1. Predicting house prices \n",
    "2. Classifying spam emails \n",
    "3. Estimating the value of a car \n",
    "4. Forecasting stock prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8d7ab",
   "metadata": {},
   "source": [
    "The correct answer is **2. Classifying spam emails** ✅\n",
    "\n",
    "Reason:\n",
    "\n",
    "* **Decision Tree Regression** is used for predicting **continuous numerical values** (house prices, car values, stock prices).\n",
    "* **Classifying spam emails** is a **classification task**, not regression, so you’d use a **Decision Tree Classifier**, not a regressor.\n",
    "\n",
    "So, this one doesn’t fit the “regression” category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325c04b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7082e56",
   "metadata": {},
   "source": [
    "## Q3. What is the primary focus of Variance Reduction in Decision Tree Regression? \n",
    "1. To increase the complexity of the tree \n",
    "2. To reduce the spread of the target variables in subsets \n",
    "3. To maximize the variance of the target variance \n",
    "4. To increase the entropy of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e40d34",
   "metadata": {},
   "source": [
    "The correct answer is **2. To reduce the spread of the target variables in subsets** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In **Decision Tree Regression**, the splitting criterion (like MSE or MAE) tries to create subsets where the target values are as close to each other as possible.\n",
    "* This means **reducing variance** within each subset so predictions become more accurate.\n",
    "* The other options don’t match regression’s goal:\n",
    "\n",
    "  * **1** increases complexity, which is not the aim.\n",
    "  * **3** is the opposite of the goal.\n",
    "  * **4** refers to entropy, which is for classification, not regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267567e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d87d0fb",
   "metadata": {},
   "source": [
    "## Q4. Which of the following is NOT a pruning technique in Decision Tree Regression? \n",
    "1. Pre-pruning \n",
    "2. Post-pruning \n",
    "3. Cost Complexity Pruning \n",
    "4. Leaf Node Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a14c39",
   "metadata": {},
   "source": [
    "The correct answer is **4. Leaf Node Pruning** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Common pruning techniques in decision trees include:\n",
    "\n",
    "  * **Pre-pruning** → stopping tree growth early based on conditions (max depth, min samples, etc.).\n",
    "  * **Post-pruning** → growing the full tree, then trimming back branches.\n",
    "  * **Cost Complexity Pruning (CCP)** → post-pruning method balancing tree size vs. error using an α parameter.\n",
    "* **Leaf Node Pruning** is **not** a standard pruning technique in decision tree regression (or classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e280",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a1caead",
   "metadata": {},
   "source": [
    "## Q5. What is the main advantage of using Mean Absolute Error (MAE) over Mean Squared Error (MSE) in Decision Tree Regression? \n",
    "1. MAE is more sensitive to large errors \n",
    "2. MAE penalizes outliers less than MSE \n",
    "3. MAE gives more weight to large errors \n",
    "4. MAE is computationally more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d9e5a",
   "metadata": {},
   "source": [
    "The correct answer is **2. MAE penalizes outliers less than MSE** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **MSE** squares the errors, so large errors (outliers) have a disproportionately high impact.\n",
    "* **MAE** takes the absolute value of errors, so it treats all errors equally and is **less sensitive** to outliers.\n",
    "* This makes MAE more robust when your data has extreme values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f877087",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae8826c",
   "metadata": {},
   "source": [
    "## Q6. In Decision Tree Regression, What is the goal of each split? \n",
    "1. To increase entropy \n",
    "2. To maximize variance \n",
    "3. To minimize variance \n",
    "4. To increase error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f0219",
   "metadata": {},
   "source": [
    "The correct answer is **3. To minimize variance** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In **Decision Tree Regression**, splits are chosen to make the target values in each subset as similar as possible.\n",
    "* This means **reducing the variance** (spread) of the target values within each subset.\n",
    "* **Entropy** is a classification metric, **maximizing variance** is the opposite of the goal, and **increasing error** is never the aim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36bfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d4f0e4",
   "metadata": {},
   "source": [
    "## Q7. Which of the following is the primary objective of a Decision Tree Regressor? \n",
    "1.  To classify data points \n",
    "2. To cluster data points \n",
    "3. To predict continuous values \n",
    "4. To reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8a6a6",
   "metadata": {},
   "source": [
    "The correct answer is **4. To predict continuous values** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Decision Tree Regressor** is designed for **regression tasks**, where the output is a **continuous numerical value** (e.g., predicting house prices, temperatures).\n",
    "* **Classification** (option 1) is for categorical outputs.\n",
    "* **Clustering** (option 2) is unsupervised learning.\n",
    "* **Dimensionality reduction** (option 4 in your list but numbered 5 here) is done by methods like PCA, not decision tree regression.\n",
    "\n",
    "Looks like your option numbering might have a small typo — the “predicts points” one seems to be a mistake in wording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e099a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df2e3b9d",
   "metadata": {},
   "source": [
    "## Q8. Which metric is used to measure the proportion of correctly identifies positive instances? \n",
    "1. Precision \n",
    "2. Recall \n",
    "3. Accuracy \n",
    "4. F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86970f3",
   "metadata": {},
   "source": [
    "The correct answer is **2. Recall** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Recall** (also called Sensitivity or True Positive Rate) measures:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$\n",
    "\n",
    "* It tells you **how many of the actual positives were correctly identified**.\n",
    "* **Precision** focuses on correctness of positive predictions,\n",
    "* **Accuracy** considers both positives and negatives,\n",
    "* **F1 Score** is the harmonic mean of precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdaf9ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce76ffb",
   "metadata": {},
   "source": [
    "## Q9. Which of the following metrics is most appropriate for a balanced binary classification problem? \n",
    "1. Precision \n",
    "2. Recall \n",
    "3. F1 Score \n",
    "4. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e467c",
   "metadata": {},
   "source": [
    "The correct answer is **4. Accuracy** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In a **balanced binary classification** problem (equal distribution of classes), **accuracy** is a reliable metric because both classes are equally represented.\n",
    "* **Precision** and **recall** are more useful for **imbalanced datasets**.\n",
    "* **F1 score** is best when you need a balance between precision and recall, usually for imbalanced cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9564b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770c258d",
   "metadata": {},
   "source": [
    "## Q10. What is the main trade-off between Precision and Recall? \n",
    "1. Increasing Precision often decreases Recall \n",
    "2. Increasing Precision often increases Recall \n",
    "3. Precision and Recall always move together \n",
    "4. There is no trade-off between Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9cbe8",
   "metadata": {},
   "source": [
    "The correct answer is **1. Increasing Precision often decreases Recall** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Precision** = proportion of predicted positives that are correct.\n",
    "* **Recall** = proportion of actual positives that are correctly identified.\n",
    "* If you **raise the threshold** for predicting something as positive, you’ll likely **increase precision** (fewer false positives) but **miss more actual positives** (lower recall).\n",
    "* Lowering the threshold does the opposite — increases recall but lowers precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64996f0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a55b3cd0",
   "metadata": {},
   "source": [
    "## Q11. Which metric gives equal importance to Precision and Recall? \n",
    "1. Accuracy \n",
    "2. F1 Score \n",
    "3. Gini Impurity \n",
    "4. Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3ebd5",
   "metadata": {},
   "source": [
    "The correct answer is **2. F1 Score** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **F1 Score** is the **harmonic mean** of precision and recall:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "* It gives **equal weight** to both precision and recall, making it useful when you need a balanced performance measure, especially in imbalanced datasets.\n",
    "* **Accuracy** looks at overall correctness,\n",
    "* **Gini Impurity** is a decision tree split metric,\n",
    "* **Specificity** measures true negative rate, not a balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c1ec3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142d0fc7",
   "metadata": {},
   "source": [
    "## Q12. When is Precision a better metric to use than Accuracy? \n",
    "1. When false negatives are more costly \n",
    "2. When the dataset is balanced \n",
    "3. When false positives are more costly \n",
    "4. When true negatives are more important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f91c6",
   "metadata": {},
   "source": [
    "The correct answer is **3. When false positives are more costly** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Precision** focuses on reducing **false positives** — it measures how many predicted positives are actually correct.\n",
    "* It’s more important than accuracy when **wrongly predicting something as positive has high cost** (e.g., flagging a non-spam email as spam, arresting an innocent person).\n",
    "* **False negatives** being costly → recall becomes more important.\n",
    "* Balanced datasets → accuracy works fine.\n",
    "* True negatives being more important → specificity is the right metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea98c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5c83a9",
   "metadata": {},
   "source": [
    "## Q13. In Decision Tree Classification, which of the following can be used as a splitting criterion? \n",
    "1. Mean Absolute Error \n",
    "2. Entropy \n",
    "3. Root Mean Square Error \n",
    "4. Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444101ce",
   "metadata": {},
   "source": [
    "The correct answer is **2. Entropy** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In **Decision Tree Classification**, common splitting criteria include:\n",
    "\n",
    "  * **Entropy** (used in Information Gain)\n",
    "  * **Gini Impurity**\n",
    "* **MAE**, **RMSE**, and **MSE** are regression metrics, not classification split criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308697b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c104e415",
   "metadata": {},
   "source": [
    "## Q14. What is the main objective of a Decision Tree Classifier? \n",
    "1. To cluster data points \n",
    "2. To predicts continuous values \n",
    "3. To classify data into predefined classes \n",
    "4. To reduce data dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3c967",
   "metadata": {},
   "source": [
    "The correct answer is **3. To classify data into predefined classes** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* A **Decision Tree Classifier** is designed for **classification tasks**, where the goal is to assign input data to one of several **predefined categories**.\n",
    "* **Clustering** (option 1) is unsupervised learning.\n",
    "* **Predicting continuous values** (option 2) is the job of a regressor.\n",
    "* **Dimensionality reduction** (option 4) is done using methods like PCA, not decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08f26b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371d8ad8",
   "metadata": {},
   "source": [
    "## Q15. In a binary classification problem, what does the diagonal elements of the confusion matrix represent? \n",
    "1. False Positives and False Negatives \n",
    "2. True Positives and True Negatives \n",
    "3. Precision and Recall \n",
    "4. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e20f56",
   "metadata": {},
   "source": [
    "The correct answer is **2. True Positives and True Negatives** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In a **confusion matrix**:\n",
    "\n",
    "  * **Diagonal elements** = **correct predictions** (True Positives + True Negatives)\n",
    "  * **Off-diagonal elements** = **incorrect predictions** (False Positives + False Negatives)\n",
    "* These diagonal values are what accuracy sums up before dividing by total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1b8c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31236617",
   "metadata": {},
   "source": [
    "## Q16. Which of the following is not a component of the confusion matrix? \n",
    "1. True Positives \n",
    "2. True Negatives \n",
    "3. False Positive Rates \n",
    "4. False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be4042",
   "metadata": {},
   "source": [
    "The correct answer is **3. False Positive Rates** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* A **confusion matrix** contains **counts**:\n",
    "\n",
    "  * **True Positives (TP)**\n",
    "  * **True Negatives (TN)**\n",
    "  * **False Positives (FP)**\n",
    "  * **False Negatives (FN)**\n",
    "* **False Positive Rate (FPR)** is **derived** from the confusion matrix but is **not itself a direct component** — it’s calculated as:\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1e037",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9a0091",
   "metadata": {},
   "source": [
    "## Q17. In Decision Tree Classification, the split with the highest Information Gain is chosen because \n",
    "1. It maximizes variance \n",
    "2. It minimizes variance \n",
    "3. It maximizes entropy \n",
    "4. It minimizes entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc514e",
   "metadata": {},
   "source": [
    "The correct answer is **4. It minimizes entropy** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Information Gain** = reduction in entropy after a split.\n",
    "* Higher Information Gain means the split makes the resulting subsets **more pure** (less disorder).\n",
    "* So, choosing the split with the **highest Information Gain** is equivalent to choosing the split that **minimizes entropy** the most.\n",
    "* **Variance** is relevant to regression, not classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2aab4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "280172b6",
   "metadata": {},
   "source": [
    "## Q18. What is the Gini Impurity value for a pure node? \n",
    "1. 0 \n",
    "2. 0.5 \n",
    "3. 1 \n",
    "4. -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379810a7",
   "metadata": {},
   "source": [
    "The correct answer is **1. 0** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Gini Impurity** measures how often a randomly chosen sample would be incorrectly classified if it were labeled according to the class distribution in the node.\n",
    "* Formula:\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 1 - \\sum p_i^2\n",
    "$$\n",
    "\n",
    "* For a **pure node** (all samples belong to one class), $p_i = 1$ for that class and $0$ for others → Gini = $1 - 1^2 = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d632b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0630bc",
   "metadata": {},
   "source": [
    "## Q19. Entropy in Decision Tree Classification is a measure of \n",
    "1. Accuracy \n",
    "2. Impurity \n",
    "3. Variance \n",
    "4. Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a827865",
   "metadata": {},
   "source": [
    "The correct answer is **2. Impurity** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In decision tree classification, **entropy** measures the **disorder or impurity** in a node.\n",
    "* A node with mixed classes has **high entropy**, and a pure node (all samples same class) has **low entropy**.\n",
    "* Accuracy, variance, and pruning are different concepts unrelated to the definition of entropy here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ace58b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c30867c2",
   "metadata": {},
   "source": [
    "## Q20. Which splitting criterion is commonly used in Decision Tree Classification? \n",
    "1. Variance reduction \n",
    "2. Information Gain \n",
    "3. Mean Squared Error \n",
    "4. Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3c412",
   "metadata": {},
   "source": [
    "The correct answer is **2. Information Gain** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Information Gain** (based on entropy) and **Gini Impurity** are the two most common splitting criteria in **Decision Tree Classification**.\n",
    "* **Variance reduction** and **MSE/RMSE** are used in regression trees, not classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d298aae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb6d34e3",
   "metadata": {},
   "source": [
    "## Q21. Which of the following is a significant advantage of  Decision Tree Calssifiers? \n",
    "1. High Complexity \n",
    "2. Requires extensive feature scaling \n",
    "3. Easy to interpret and visualize \n",
    "4. Works only with numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57f13",
   "metadata": {},
   "source": [
    "The correct answer is **3. Easy to interpret and visualize** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Decision Tree Classifiers are popular because their decision-making process can be easily visualized as a tree structure, making them very interpretable.\n",
    "* They **do not require extensive feature scaling** (like normalization).\n",
    "* They handle both **numerical and categorical data**.\n",
    "* High complexity is usually a disadvantage, not an advantage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a50f11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55bc12cc",
   "metadata": {},
   "source": [
    "## Q22. Which loss function is commonly used in Decision Tree Regression? \n",
    "1. Gini Impurity \n",
    "2. Information Gain \n",
    "3. Mean Squared Error (MSE) \n",
    "4. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a15d8a",
   "metadata": {},
   "source": [
    "The correct answer is **3. Mean Squared Error (MSE)** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In **Decision Tree Regression**, **MSE** (or sometimes MAE) is commonly used as the loss function to measure how well the tree fits the data.\n",
    "* **Gini Impurity** and **Information Gain** are used in classification trees.\n",
    "* **Precision** is a classification metric, not a loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629c149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "995dc446",
   "metadata": {},
   "source": [
    "## Q23. In Decision Tree Regression, pruning is used to \n",
    "1. Increase tree depth \n",
    "2. Reduce overfitting \n",
    "3. Increase variance \n",
    "4. Increase bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd13d3a",
   "metadata": {},
   "source": [
    "The correct answer is **2. Reduce overfitting** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Pruning** simplifies the tree by removing branches that have little power in predicting the target, which helps to **reduce overfitting**.\n",
    "* It usually **decreases variance** and may **increase bias** slightly, but the main goal is to improve generalization.\n",
    "* Increasing tree depth or variance is the opposite of pruning’s effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90488677",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8861fd",
   "metadata": {},
   "source": [
    "## Q24. In Decision Tree Regression, which metric is used to evaluate the quality of a split? \n",
    "1. Gini Index \n",
    "2. Entropy \n",
    "3. Mean Squared Error (MSE) \n",
    "4. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02e5f3",
   "metadata": {},
   "source": [
    "The correct answer is **3. Mean Squared Error (MSE)** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* In **Decision Tree Regression**, the quality of a split is evaluated by how much it **reduces the Mean Squared Error** within the subsets.\n",
    "* **Gini Index** and **Entropy** are used in classification trees, not regression.\n",
    "* **Precision** is a classification metric, not used for evaluating splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123e613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78bed311",
   "metadata": {},
   "source": [
    "## Q25. Which pruning technique involves stopping the growth of a tree before it becomes too complex? \n",
    "1. Pre-pruning \n",
    "2. Post-pruning \n",
    "3. Cost Complexity Pruning \n",
    "4. Leaf Node Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbdccd",
   "metadata": {},
   "source": [
    "The correct answer is **1. Pre-pruning** ✅\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Pre-pruning** (also called early stopping) stops the tree from growing further based on conditions like maximum depth, minimum samples per leaf, or minimum impurity decrease.\n",
    "* **Post-pruning** and **Cost Complexity Pruning** occur after the full tree is grown, trimming branches back.\n",
    "* **Leaf Node Pruning** is not a standard term in pruning methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3de6bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21adfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc113d9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2bfb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
