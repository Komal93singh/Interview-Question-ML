{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ef9828",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2473ebfb",
   "metadata": {},
   "source": [
    "# QUIZ: SUPPORT VECTOR MACHINE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fea48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f85990",
   "metadata": {},
   "source": [
    "## Q1. Which kernel is most suitable for capturing complex, non-linear relationships in data? \n",
    "1. Linear kernel \n",
    "2. Polynomial kernel \n",
    "3. Gaussian (RBF) kernel \n",
    "4. identity kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e602bee",
   "metadata": {},
   "source": [
    "The most suitable kernel for capturing complex, non-linear relationships in data is:\n",
    "\n",
    "**3. Gaussian (RBF) kernel**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The **Gaussian Radial Basis Function (RBF)** kernel can model very flexible and complex non-linear patterns by mapping input features into an infinite-dimensional space.\n",
    "* The **Linear kernel** works well for linear relationships only.\n",
    "* The **Polynomial kernel** can capture some non-linearity but might be limited for very complex data unless you use a very high degree.\n",
    "* The **Identity kernel** is not commonly used and does not transform the data to capture non-linearity.\n",
    "\n",
    "So, **Gaussian (RBF) kernel** is the best choice here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eba0c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954bdce7",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using kernels in SVM over explicitly mapping the data to a higher-dimensional space? \n",
    "1. It allows for non-linear transformations without explicitly computing high-dimensional coordinates \n",
    "2. It reduces the number of support vectors \n",
    "3. It simplifies the optimization process \n",
    "4. It automatically selects the best features for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462be958",
   "metadata": {},
   "source": [
    "The main advantage of using kernels in SVM over explicitly mapping the data to a higher-dimensional space is:\n",
    "\n",
    "**1. It allows for non-linear transformations without explicitly computing high-dimensional coordinates**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Kernel functions let SVM implicitly operate in a high-dimensional feature space without the costly computation of coordinates in that space (called the **kernel trick**).\n",
    "* This avoids the computational burden and memory cost of explicit mapping, making it efficient.\n",
    "* Options 2, 3, and 4 are not the primary advantages of kernels.\n",
    "\n",
    "So, **option 1** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed50d74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c59ded12",
   "metadata": {},
   "source": [
    "## Q3. What is the main purpose of the Lagrange multiplier technique in SVM optimization? \n",
    "1. To introduce non-linearity in the model \n",
    "2. To convert the constrained optimization problem into an unconstrained one \n",
    "3. To compute the kernel function \n",
    "4. To select the best features for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870f7f8",
   "metadata": {},
   "source": [
    "The main purpose of the Lagrange multiplier technique in SVM optimization is:\n",
    "\n",
    "**2. To convert the constrained optimization problem into an unconstrained one**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* SVM optimization involves maximizing the margin with constraints (like correct classification).\n",
    "* The Lagrange multipliers help reformulate this constrained problem into a form that can be solved more easily using methods for unconstrained optimization.\n",
    "* This approach also helps identify support vectors via non-zero multipliers.\n",
    "* Options 1, 3, and 4 are unrelated to the role of Lagrange multipliers.\n",
    "\n",
    "So, **option 2** is the correct answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c4112",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd57ddf",
   "metadata": {},
   "source": [
    "## Q4. What does the gamma parameter control in the Gaussian (RBF) kernel? \n",
    "1. The degree of the polynomial \n",
    "2. The width of the Gaussian function \n",
    "3. The offset from the origin \n",
    "4. The number of support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c773d",
   "metadata": {},
   "source": [
    "The gamma parameter in the Gaussian (RBF) kernel controls:\n",
    "\n",
    "**2. The width of the Gaussian function**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Gamma defines how far the influence of a single training example reaches.\n",
    "* A small gamma means a large variance (wide Gaussian), so points far away can influence the decision boundary.\n",
    "* A large gamma means a narrow Gaussian, so only points very close influence the decision boundary, leading to more complex models.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3694e9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "160ced77",
   "metadata": {},
   "source": [
    "## Q5. What is the primary advantage of solving the dual problem in SVM optimization? \n",
    "1. It always results in a global optimum \n",
    "2. It eliminates the need for kernel functions \n",
    "3. It is often more efficients for large datasets \n",
    "4. It reduces the number of support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146f41b",
   "metadata": {},
   "source": [
    "The primary advantage of solving the dual problem in SVM optimization is:\n",
    "\n",
    "**1. It always results in a global optimum**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The dual formulation of SVM is a convex quadratic optimization problem, guaranteeing a global optimum.\n",
    "* Also, solving the dual allows the use of kernel functions to handle non-linear decision boundaries efficiently (so kernel functions *are* used, not eliminated).\n",
    "* While the dual can sometimes be more efficient, especially with kernels, it’s not always more efficient for large datasets compared to primal methods.\n",
    "* The number of support vectors is determined by the data, not by choosing primal or dual.\n",
    "\n",
    "So, the best answer is **option 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de2b81",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba4b96cf",
   "metadata": {},
   "source": [
    "## Q6. What is the primary difference between hard margin and soft margin SVM? \n",
    "1. Hard margin uses kernel functions, while soft margin doesn't \n",
    "2. Soft margin allows for some misclassification, while hard margin doesn't \n",
    "3. Hard margin is used for regression, while soft margin is for classification \n",
    "4. Soft margin reduces computational complexity compared to hard margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef650fd",
   "metadata": {},
   "source": [
    "The primary difference between hard margin and soft margin SVM is:\n",
    "\n",
    "**2. Soft margin allows for some misclassification, while hard margin doesn't**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **Hard margin SVM** requires that all training data points are perfectly separated with no errors (no misclassification), which is only possible if data is linearly separable.\n",
    "* **Soft margin SVM** introduces slack variables allowing some misclassification or margin violations to handle non-separable or noisy data better.\n",
    "* Both can use kernels, both are for classification (not regression), and soft margin doesn't necessarily reduce computational complexity.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5e88b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b61ae76c",
   "metadata": {},
   "source": [
    "## Q7. In SVM Regression, what is the purpose of the epsilon-insensitive loss function? \n",
    "1. To maximize the margin between classes \n",
    "2. To ignore errors within a specified margin \n",
    "3. To transform the input space \n",
    "4. To compute the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1016f37",
   "metadata": {},
   "source": [
    "In SVM Regression, the purpose of the epsilon-insensitive loss function is:\n",
    "\n",
    "**2. To ignore errors within a specified margin**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The epsilon-insensitive loss allows the model to ignore errors (differences between predicted and actual values) that fall within a margin of size epsilon.\n",
    "* This helps create a \"tube\" around the regression function where small deviations are not penalized, promoting a simpler model that doesn’t try to fit noise.\n",
    "* Options 1, 3, and 4 are unrelated to this loss function.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cbaba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "966b2224",
   "metadata": {},
   "source": [
    "## Q8. In the polynomial kernel function K(x,y) = (x.y+c)^d, what does the parameter d represent? \n",
    "1. The distance between x and y \n",
    "2. The dimensionality of the input space \n",
    "3. The degree of the polynomial \n",
    "4. The regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77619611",
   "metadata": {},
   "source": [
    "In the polynomial kernel function $K(x, y) = (x \\cdot y + c)^d$, the parameter **d** represents:\n",
    "\n",
    "**3. The degree of the polynomial**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The degree $d$ controls the flexibility of the polynomial kernel and how complex the decision boundary can be.\n",
    "* It determines the power to which the dot product (plus constant $c$) is raised.\n",
    "* It is *not* the distance, input dimensionality, or regularization parameter.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a162fa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b90f2b9f",
   "metadata": {},
   "source": [
    "## Q9. What is the amin limitation of Linear SVM? \n",
    "1. It cannot handle high-dimensional data \n",
    "2. It is computationally more expensive than non-linear SVM \n",
    "3. It cannot separate classes that are not linearly separable \n",
    "4. It always overfits the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaaa2b8",
   "metadata": {},
   "source": [
    "The main limitation of Linear SVM is:\n",
    "\n",
    "**3. It cannot separate classes that are not linearly separable**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Linear SVM can only find a linear decision boundary, so if the classes are not linearly separable, it won't perform well.\n",
    "* Linear SVM actually handles high-dimensional data well and is usually computationally less expensive than non-linear SVM.\n",
    "* It does not always overfit; in fact, linear SVM is often less prone to overfitting compared to more complex models.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bfc7e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bcda048",
   "metadata": {},
   "source": [
    "## Q10. In the context of SVM, what does the term \"maximum margin hyperplane\" refer to? \n",
    "1. The hyperplane with the most support vectors \n",
    "2. The hyperplane that perfectly separates all data points \n",
    "3. The hyperplane that has the largest distance from the nearest data points of both classes \n",
    "4. The hyperplane with the highest dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd0141",
   "metadata": {},
   "source": [
    "In the context of SVM, the term **\"maximum margin hyperplane\"** refers to:\n",
    "\n",
    "**3. The hyperplane that has the largest distance from the nearest data points of both classes**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* SVM aims to find the hyperplane that maximizes the margin, i.e., the distance between the hyperplane and the closest data points (support vectors) from each class.\n",
    "* This maximizes the model's generalization ability.\n",
    "* It’s not necessarily the one with the most support vectors, nor does it always perfectly separate all points (especially in soft margin SVM), and dimensionality refers to the feature space, not the hyperplane.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d6e3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06001848",
   "metadata": {},
   "source": [
    "## Q11. What is the primary advantage of using the polynomial kernel over the linear kernel? \n",
    "1. It always results in faster computation \n",
    "2. It can capture polynomial relationships between features \n",
    "3. It reduces the dimensionality of the data \n",
    "4. It eliminates the need for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea731a",
   "metadata": {},
   "source": [
    "The primary advantage of using the polynomial kernel over the linear kernel is:\n",
    "\n",
    "**2. It can capture polynomial relationships between features**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The polynomial kernel allows the SVM to model non-linear relationships by mapping features into a higher-dimensional space where polynomial relationships can be captured.\n",
    "* It is usually more computationally expensive than a linear kernel (so option 1 is wrong).\n",
    "* It does not reduce dimensionality (option 3), rather it implicitly increases it.\n",
    "* It does not eliminate the need for regularization (option 4).\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fb78b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be87f678",
   "metadata": {},
   "source": [
    "## Q12. Which of the following is NOT a common application of the linear kernels in SVM? \n",
    "1. Text classification \n",
    "2. Image classification with complex patterns \n",
    "3. High-dimensional gene expression data \n",
    "4. Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fe7ed",
   "metadata": {},
   "source": [
    "The option that is **NOT** a common application of linear kernels in SVM is:\n",
    "\n",
    "**2. Image classification with complex patterns**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Linear kernels work well for **high-dimensional** and **sparse data** like text classification, spam detection, and gene expression data where the data is often linearly separable or close to it.\n",
    "* For **image classification with complex patterns**, non-linear kernels (like RBF or polynomial) are preferred because they can capture complex, non-linear relationships in pixel data.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c026975",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0126d189",
   "metadata": {},
   "source": [
    "## Q13. What is the purpose of slack variables in soft margin classification? \n",
    "1. To increase the margin size \n",
    "2. To measure the degree of misclassification \n",
    "3. To transform the feature space \n",
    "4. To compute the kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14ef0d",
   "metadata": {},
   "source": [
    "The purpose of slack variables in soft margin classification is:\n",
    "\n",
    "**2. To measure the degree of misclassification**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Slack variables allow some data points to lie inside the margin or be misclassified, quantifying how much each point violates the margin constraints.\n",
    "* This helps the model balance between maximizing the margin and minimizing classification errors in non-linearly separable data.\n",
    "* Slack variables don’t transform the feature space, increase margin size, or compute kernels.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fd544",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "656d64ce",
   "metadata": {},
   "source": [
    "## Q14. In SVM Regression, what does the epsilon parameter represent? \n",
    "1. The learning rate \n",
    "2. The margin of tolerance for errors \n",
    "3. The number of support vectors \n",
    "4. The degree of the polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2fd2c",
   "metadata": {},
   "source": [
    "In SVM Regression, the epsilon parameter represents:\n",
    "\n",
    "**2. The margin of tolerance for errors**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Epsilon defines a tube around the regression function where errors (differences between predicted and actual values) are ignored.\n",
    "* It sets how much deviation is tolerated without penalty, controlling the model’s sensitivity to small errors.\n",
    "* It is not the learning rate, number of support vectors, or degree of the polynomial kernel.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4200c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8c32886",
   "metadata": {},
   "source": [
    "## Q15. What is the main difference between SVM classification and SVM regression? \n",
    "1. SVM Regression uses kernel functions, while Classification doesn't \n",
    "2. SVM Classificiation maximizes margin, while Regression minimizes it \n",
    "3. SVM Classification predicts discrete classes, while Regression predicts continuous values \n",
    "4. SVM Regression is always linear, while Classificiation can be non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35286f6b",
   "metadata": {},
   "source": [
    "The main difference between SVM classification and SVM regression is:\n",
    "\n",
    "**3. SVM Classification predicts discrete classes, while Regression predicts continuous values**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* SVM **classification** assigns input data to discrete categories or classes.\n",
    "* SVM **regression** predicts continuous numerical values.\n",
    "* Both can use kernel functions, both can be linear or non-linear, and both involve margin concepts but tailored to classification or regression.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c9341",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f2a753",
   "metadata": {},
   "source": [
    "## Q16. What is the primary goal of Linear SVM in classification tasks? \n",
    "1. To MInimize the margin between classes \n",
    "2. To maximizes the margin between classes \n",
    "3. To create a curved decision boundary \n",
    "4. To minimize the number of support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827427c9",
   "metadata": {},
   "source": [
    "The primary goal of Linear SVM in classification tasks is:\n",
    "\n",
    "**2. To maximize the margin between classes**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Linear SVM tries to find the hyperplane that maximizes the distance (margin) between the closest points of different classes, improving generalization.\n",
    "* It does **not** minimize the margin, nor does it create curved boundaries (that’s for non-linear kernels).\n",
    "* It does not explicitly aim to minimize the number of support vectors.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe6bf0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8b93f0",
   "metadata": {},
   "source": [
    "## Q17. What is the \"kernel trick\" in SVM? \n",
    "1. A method to reduce computational complixty \n",
    "2. A technique to explicitly compute coodinates in high-dimensional space \n",
    "3. A way to implicitly compute inner products in high-dimensional space \n",
    "4. A procedure to eliminate irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab811f43",
   "metadata": {},
   "source": [
    "The \"kernel trick\" in SVM is:\n",
    "\n",
    "**3. A way to implicitly compute inner products in high-dimensional space**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The kernel trick allows SVM to calculate the similarity (inner product) between data points in a high-dimensional feature space without explicitly computing their coordinates.\n",
    "* This makes it efficient to handle non-linear relationships without the computational cost of explicit mapping.\n",
    "* It is not about reducing computational complexity directly, explicitly computing coordinates, or feature elimination.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f97f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b87312b4",
   "metadata": {},
   "source": [
    "## Q18. Which kernel function is defined as K(x,y) = x.y? \n",
    "1. Polynomial kernel \n",
    "2. Gaussian (RBF) kernel \n",
    "3. Sigmoid kernel \n",
    "4. Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40b459",
   "metadata": {},
   "source": [
    "The kernel function defined as $K(x, y) = x \\cdot y$ is:\n",
    "\n",
    "**4. Linear kernel**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The linear kernel is simply the dot product between vectors $x$ and $y$.\n",
    "* Polynomial, Gaussian (RBF), and Sigmoid kernels have more complex formulas involving powers, exponentials, or sigmoid functions.\n",
    "\n",
    "So, **option 4** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ab16d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e87b91",
   "metadata": {},
   "source": [
    "## Q19. What is the main purpose of using kernel functions in SVM? \n",
    "1. To reduce the dimensionality of the input space\n",
    "2. To increase the number of support vectors \n",
    "3. To transform data into a higher-dimensional space for better separation \n",
    "4. To eliminate the need for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545bb364",
   "metadata": {},
   "source": [
    "The main purpose of using kernel functions in SVM is:\n",
    "\n",
    "**3. To transform data into a higher-dimensional space for better separation**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Kernel functions implicitly map input data into a higher-dimensional space where classes are more likely to be linearly separable.\n",
    "* They do **not** reduce dimensionality (usually they increase it implicitly), nor do they aim to increase support vectors or eliminate optimization.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ed938",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c327fe",
   "metadata": {},
   "source": [
    "## Q20. WHat does the parameter C control in Soft Margin SVM? \n",
    ". The number of support vectors \n",
    "2. The dimensionality of the feature space \n",
    "3. The trade-off between margin size and classification error \n",
    "4. The kernel function type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b389c",
   "metadata": {},
   "source": [
    "The parameter **C** in Soft Margin SVM controls:\n",
    "\n",
    "**3. The trade-off between margin size and classification error**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* **C** determines how much the model penalizes misclassifications.\n",
    "* A large **C** tries to minimize classification errors, possibly at the cost of a smaller margin (less tolerance for errors).\n",
    "* A small **C** allows a larger margin with more misclassifications tolerated (more regularization).\n",
    "* It does not control the number of support vectors, feature space dimensionality, or kernel type.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0b18f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eece41ff",
   "metadata": {},
   "source": [
    "## Q21. What is the primary advantage of Soft Margin Classification in SVM? \n",
    "1. It always results in perfect classification \n",
    "2. It allows for some misclassification to handle non-linearly separable data \n",
    "3. It eliminates the need for support vectors \n",
    "4. It reduces the computational complexity of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb383ea",
   "metadata": {},
   "source": [
    "The primary advantage of Soft Margin Classification in SVM is:\n",
    "\n",
    "**2. It allows for some misclassification to handle non-linearly separable data**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Soft margin SVM permits some errors (misclassifications) to create a more flexible and robust model, especially when data is noisy or not perfectly separable.\n",
    "* It does **not** guarantee perfect classification, nor does it eliminate support vectors or necessarily reduce computational complexity.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32008ec2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d16f2c",
   "metadata": {},
   "source": [
    "## Q22. What does the equation w.x+b=0 represent in Linear SVM? \n",
    "1. The margin \n",
    "2. The hyperplane \n",
    "3. the support vectors \n",
    "4. The optimization function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc844926",
   "metadata": {},
   "source": [
    "The equation $w \\cdot x + b = 0$ in Linear SVM represents:\n",
    "\n",
    "**2. The hyperplane**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* This equation defines the decision boundary (hyperplane) that separates the classes in the feature space.\n",
    "* The margin refers to the distance around this hyperplane, support vectors are the points closest to this hyperplane, and the optimization function is what SVM tries to solve to find the best $w$ and $b$.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb13928",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f65fdc",
   "metadata": {},
   "source": [
    "## Q23. In the context of SVM, what are support vectors? \n",
    "1. The weight vector of the hyperplane \n",
    "2. The bias term of the hyperplane \n",
    "3. The nearest data points to the hyperplane \n",
    "4. All data points in the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791b4fc",
   "metadata": {},
   "source": [
    "In the context of SVM, **support vectors** are:\n",
    "\n",
    "**3. The nearest data points to the hyperplane**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Support vectors are the data points that lie closest to the decision boundary (hyperplane) and directly influence its position and orientation.\n",
    "* They are critical in defining the margin and the final model.\n",
    "* They are not the weight vector, bias term, or all training data points.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6faaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94fcee9b",
   "metadata": {},
   "source": [
    "## Q24. What is the primary advantage of using the Gaussian (RBF) kernel in SVM? \n",
    "1. It always results in perfect classification \n",
    "2. It can map data to an infinte-dimensional space \n",
    "3. It reduces the computational complexity of SVM \n",
    "4. It eliminates the nee for parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b37ab",
   "metadata": {},
   "source": [
    "The primary advantage of using the Gaussian (RBF) kernel in SVM is:\n",
    "\n",
    "**2. It can map data to an infinite-dimensional space**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The RBF kernel implicitly maps input data into an infinite-dimensional feature space, allowing SVM to model very complex, non-linear relationships.\n",
    "* It does **not** guarantee perfect classification, nor does it reduce computational complexity or eliminate the need for parameter tuning (like gamma and C).\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68e68e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a8a4b7",
   "metadata": {},
   "source": [
    "## Q25. Which of the 'following is TRUE about the bias term b in the SVM hyperplane equation? \n",
    "1. It is always positive \n",
    "2. It determines the orientation of the hyperplane \n",
    "3. It represents the offset of the hyperplane from the origin \n",
    "4. It is directly proportional to th emargin size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1610b1",
   "metadata": {},
   "source": [
    "The correct statement about the bias term **b** in the SVM hyperplane equation is:\n",
    "\n",
    "**3. It represents the offset of the hyperplane from the origin**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The bias $b$ shifts the hyperplane away from the origin.\n",
    "* It does **not** determine orientation (that’s controlled by the weight vector $w$), it can be positive or negative, and it is not directly proportional to the margin size.\n",
    "\n",
    "So, **option 3** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fde12b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca590b6b",
   "metadata": {},
   "source": [
    "## Q26. In the context of SVM, what does the term \"primal problem\" refer to? \n",
    "1. The original optimization problem formulation \n",
    "2. The dual form of the optimization problem \n",
    "3. The process of selecting the best kernel function \n",
    "4. The method of computing support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f27d1",
   "metadata": {},
   "source": [
    "In the context of SVM, the term **\"primal problem\"** refers to:\n",
    "\n",
    "**1. The original optimization problem formulation**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The primal problem is the initial constrained optimization problem that SVM aims to solve to find the best separating hyperplane.\n",
    "* The dual problem is a reformulation of this problem that often makes it easier to solve, especially with kernels.\n",
    "* Options 3 and 4 are unrelated to the primal problem.\n",
    "\n",
    "So, **option 1** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787ed50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c721ad5",
   "metadata": {},
   "source": [
    "## Q27. Which of the following is NOT a characteristic of support vectors in SVM? \n",
    "1. They are the data points closest to the decision boundary \n",
    "2. They determine the position of the hyperplane \n",
    "3. They are always correctly classified \n",
    "4. They are crucial for defining the amrgin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928c351",
   "metadata": {},
   "source": [
    "The statement that is **NOT** a characteristic of support vectors in SVM is:\n",
    "\n",
    "**3. They are always correctly classified**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* Support vectors are indeed the points closest to the decision boundary (option 1), they determine the hyperplane's position (option 2), and are crucial for defining the margin (option 4).\n",
    "* However, in **soft margin SVM**, some support vectors can be misclassified (inside the margin or on the wrong side), so they are **not always correctly classified**.\n",
    "\n",
    "So, **option 3** is correct as the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a224315",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec59f6d0",
   "metadata": {},
   "source": [
    "## Q28. Which of the following statements about SVM Regression is FALSE? \n",
    "1. It uses an epsilon-insensitive loss function \n",
    "2. It aims to find a function that fits all data points perfectly \n",
    "3. It can use different kernel functions for non-linear regression \n",
    ". It tries to keep the regression function as flat as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ed60d",
   "metadata": {},
   "source": [
    "The **FALSE** statement about SVM Regression is:\n",
    "\n",
    "**2. It aims to find a function that fits all data points perfectly**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* SVM regression uses an epsilon-insensitive loss function, which **does not** penalize errors within a margin, so it does **not** try to fit all data points perfectly.\n",
    "* It can use kernels for non-linear regression (option 3), and it tries to keep the function flat (option 4) to avoid overfitting.\n",
    "\n",
    "So, **option 2** is false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba80b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19039f5b",
   "metadata": {},
   "source": [
    "## Q29. In the context of SVM, what does the term \"feature space\" refer to ? \n",
    "1. The original space of input features \n",
    "2. The transformed space after applying a kernel function \n",
    "3. The space of support vectors \n",
    "4. The space of slack variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ec72c",
   "metadata": {},
   "source": [
    "In the context of SVM, the term **\"feature space\"** refers to:\n",
    "\n",
    "**2. The transformed space after applying a kernel function**\n",
    "\n",
    "Explanation:\n",
    "\n",
    "* The feature space is where the original input data is mapped (often to a higher dimension) by the kernel function to make it easier to separate classes linearly.\n",
    "* It is different from the original input space (option 1), and not related to the space of support vectors or slack variables.\n",
    "\n",
    "So, **option 2** is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f420ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f04c7f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b20694",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb505fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
