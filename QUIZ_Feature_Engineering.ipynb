{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021a485",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dc081e6",
   "metadata": {},
   "source": [
    "# QUIZ : FEATURE ENGINEERING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ca4cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c208f2d",
   "metadata": {},
   "source": [
    "## 1. Why is feature scaling important when using gradient descent-based algorithms? \n",
    "1. It improves the convergence speed of the algorithm \n",
    "2. it eliminates the need for data cleaning \n",
    "3. It reduces the dimensionality of the data \n",
    "4. It ensures that the features are interdependent of each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c00471",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. It improves the convergence speed of the algorithm**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "Feature scaling (like normalization or standardization) is **important in gradient descent-based algorithms** because:\n",
    "\n",
    "* Gradient descent updates weights based on the **magnitude of feature values**.\n",
    "* If features have very different scales (e.g., one ranges from 1 to 1000, another from 0 to 1), the **cost function becomes skewed**, leading to **slow or unstable convergence**.\n",
    "* Scaling makes the **optimization landscape smoother**, allowing the algorithm to take more **consistent and efficient steps** toward the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **2. It eliminates the need for data cleaning** – Data cleaning involves handling missing values, duplicates, outliers, etc., and is unrelated to scaling.\n",
    "* **3. It reduces the dimensionality of the data** – Dimensionality reduction (e.g., PCA) is a separate process from scaling.\n",
    "* **4. It ensures that the features are interdependent of each other** – Scaling does **not affect the relationships (dependency/independency)** between features.\n",
    "\n",
    "---\n",
    "\n",
    "**So, always scale your features when using gradient descent-based algorithms (like linear regression, logistic regression, neural networks, etc.)!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01387507",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bf80c1",
   "metadata": {},
   "source": [
    "## 2. When should you consider using robust scaling? \n",
    "1. When your data is normally distributed \n",
    "2. When your data contains significant outliers \n",
    "3. When you need to normalize data to a 0-1 range \n",
    "4. When your data is categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29609c",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. When your data contains significant outliers**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Robust scaling** (like using **RobustScaler** in scikit-learn) **uses the median and interquartile range (IQR)** for scaling:\n",
    "\n",
    "* It is **less sensitive to outliers** than standard scaling (which uses mean and standard deviation).\n",
    "* It transforms the data by **subtracting the median and dividing by the IQR**.\n",
    "* This makes it ideal when your dataset has **extreme values or outliers** that would distort other scaling methods.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. When your data is normally distributed** – Use **StandardScaler** (mean = 0, std = 1), not robust scaling.\n",
    "* **3. When you need to normalize data to a 0-1 range** – Use **MinMaxScaler**, not robust scaling.\n",
    "* **4. When your data is categorical** – Categorical data should be **encoded** (e.g., one-hot or label encoding), not scaled.\n",
    "\n",
    "---\n",
    "\n",
    "**RobustScaler = Best choice when outliers are present.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a6d13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eae088c5",
   "metadata": {},
   "source": [
    "## 3. Which of the following encoding techniques is most suitable for high-cardinality categorical variables? \n",
    "1. Label Encoding \n",
    "2. One-Hot Encoding \n",
    "3. Binary Encoding \n",
    "4. Frequency Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ab2e9",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Binary Encoding**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Binary Encoding** is most suitable for **high-cardinality categorical variables** (i.e., variables with a large number of unique categories) because:\n",
    "\n",
    "* It **reduces the number of dimensions** compared to One-Hot Encoding.\n",
    "* It works by first converting categories to numeric labels (like Label Encoding), then encoding those numbers into **binary format**, and finally splitting binary digits into separate columns.\n",
    "* This results in **fewer columns** than One-Hot Encoding and avoids the **ordinal assumptions** of Label Encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are less suitable:\n",
    "\n",
    "* **1. Label Encoding** – Imposes an **ordinal relationship** which can mislead algorithms if the categories are **nominal**.\n",
    "* **2. One-Hot Encoding** – Creates **a new column for each unique category**, leading to **sparse and high-dimensional data**, which is inefficient for high-cardinality features.\n",
    "* **4. Frequency Encoding** – Replaces categories with their frequency, but may **mislead models** if categories with similar frequencies don’t carry similar meaning.\n",
    "\n",
    "---\n",
    "\n",
    "**Use Binary Encoding when working with high-cardinality categorical features to balance efficiency and accuracy.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988b0e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c71a233",
   "metadata": {},
   "source": [
    "## 4. What is the main risk of using one-hot encoding on a categorical variable with a very high number of categories? \n",
    "1. It can lead to data sparsity \n",
    "2. It simplifies the data too much \n",
    "3. It increases the likelihood of multicollinearity \n",
    "4. It reduces the interpretability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161becb2",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. It can lead to data sparsity**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**One-Hot Encoding** creates **one binary column for each category** in the feature. When the categorical variable has a **very high number of unique categories (high cardinality)**:\n",
    "\n",
    "* It results in a **large number of columns**, most of which contain **zeros**.\n",
    "* This creates a **sparse matrix**, meaning a lot of memory is used to store mostly zero values.\n",
    "* Sparse data can **slow down model training**, increase **computational cost**, and even lead to **overfitting** in some models.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **2. It simplifies the data too much** – One-hot encoding actually **adds complexity**, not simplification.\n",
    "* **3. It increases the likelihood of multicollinearity** – One-hot encoding avoids multicollinearity if you **drop one dummy variable** per feature (i.e., avoid the dummy variable trap).\n",
    "* **4. It reduces the interpretability of the model** – Interpretability may decrease slightly with more features, but **this is not the main risk** compared to sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:**\n",
    "Use one-hot encoding **cautiously** with high-cardinality variables — consider alternatives like **binary or target encoding** in such cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703cc2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8bbb08",
   "metadata": {},
   "source": [
    "## 5. How does feature hashing differ from one-hot encoding? \n",
    "1. Feature hashing creates fewer columns by using a hash function \n",
    "2. Feature hashing is only applicable to numerical data \n",
    "3. Feature hashing increases the dimensionality of the dataset \n",
    "4. Feature hashing provides better interpretability than one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec820",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. Feature hashing creates fewer columns by using a hash function**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Feature Hashing** (also known as the **hashing trick**) is an encoding technique that:\n",
    "\n",
    "* Uses a **hash function to map categories to a fixed number of columns** (i.e., features).\n",
    "* This makes it **memory-efficient** and ideal for **high-cardinality categorical variables**, where one-hot encoding would create too many columns.\n",
    "* Unlike one-hot encoding, **the number of output features is fixed** and predefined, regardless of how many unique categories are in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **2. Feature hashing is only applicable to numerical data** – Incorrect. Feature hashing is used **specifically for categorical data**.\n",
    "* **3. Feature hashing increases the dimensionality of the dataset** – Wrong. It actually **reduces** dimensionality compared to one-hot encoding.\n",
    "* **4. Feature hashing provides better interpretability than one-hot encoding** – False. **One-hot encoding is more interpretable**, since each column clearly represents a category. Feature hashing can **cause collisions**, making interpretation difficult.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Feature hashing is a **dimensionality-reducing** encoding method that trades off interpretability for **efficiency** — great for large-scale or streaming categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3ec84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa98938",
   "metadata": {},
   "source": [
    "## 6. Which of the following is NOT a common method for detecting outliers? \n",
    "1. Box Plot \n",
    "2. Z-score \n",
    "3. Isolation Forest \n",
    "4. Min-Max Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5ab38",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Min-Max Scaling**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Min-Max Scaling** is a **feature scaling technique**, not a method for detecting outliers. It transforms features to a fixed range (usually 0 to 1) but:\n",
    "\n",
    "* It does **not detect or identify** outliers.\n",
    "* In fact, it can be **heavily affected by outliers**, as extreme values can **compress** the rest of the data into a narrow range.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ The other options are **commonly used for outlier detection**:\n",
    "\n",
    "* **1. Box Plot** – Visual tool that uses the **IQR rule** to identify outliers as points beyond 1.5×IQR from Q1 and Q3.\n",
    "* **2. Z-score** – Detects outliers by measuring how many **standard deviations** a data point is from the mean (typically, |z| > 3 is considered an outlier).\n",
    "* **3. Isolation Forest** – An **unsupervised machine learning algorithm** that isolates anomalies using random decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:**\n",
    "**Min-Max Scaling is used for normalization, not for outlier detection.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42017146",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f91aa8",
   "metadata": {},
   "source": [
    "## 7. What is the purpose of using Winsorization to handle outliers? \n",
    "1. To remove outliers from the dataset entirely \n",
    "2. To replace outliers with a constant value \n",
    "3. To reduce the impact of outliers by limiting extreme values \n",
    "4. To impute missing values with the mean or median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6b5e3",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. To reduce the impact of outliers by limiting extreme values**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Winsorization** is a technique used to **handle outliers** by:\n",
    "\n",
    "* **Limiting extreme values** in the data to a specified percentile range.\n",
    "* Instead of removing outliers, it **replaces them with the nearest value within the allowed range** (e.g., replacing values above the 95th percentile with the 95th percentile value).\n",
    "\n",
    "This helps in:\n",
    "\n",
    "* **Reducing the influence** of extreme values without deleting data.\n",
    "* **Improving model robustness** and reducing skewness.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. To remove outliers from the dataset entirely** – That’s **outlier removal**, not Winsorization.\n",
    "* **2. To replace outliers with a constant value** – That’s not Winsorization; it replaces them with **threshold values**, not constants.\n",
    "* **4. To impute missing values with the mean or median** – That’s **imputation**, unrelated to outlier handling.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Winsorization = Capping extreme values to reduce outlier impact without losing data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097677a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd50c1b2",
   "metadata": {},
   "source": [
    "## 8. In feature scaling, which technique transforms the data to have a mean of 0 and a standard deviation of 1? \n",
    "1. Min-Max Scaling \n",
    "2. Robust Scaling \n",
    "3. Standardization \n",
    "4. Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa0980",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Standardization**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Standardization** (also called **Z-score normalization**) transforms the data so that:\n",
    "\n",
    "* The **mean becomes 0**\n",
    "* The **standard deviation becomes 1**\n",
    "\n",
    "The formula used is:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x$ = original value\n",
    "* $\\mu$ = mean of the feature\n",
    "* $\\sigma$ = standard deviation of the feature\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Min-Max Scaling** – Transforms data to a fixed range (typically \\[0, 1]), but does **not standardize mean or std. deviation**.\n",
    "* **2. Robust Scaling** – Uses **median and IQR** for scaling; useful for data with outliers, but doesn’t force mean = 0 or std = 1.\n",
    "* **4. Log Transformation** – Used to **reduce skewness** in distributions, not to standardize data.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Standardization = mean 0, standard deviation 1** — commonly used in algorithms like linear regression, SVMs, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961eb82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de88bdde",
   "metadata": {},
   "source": [
    "## 9. Which feature extraction technique is most suitable for visualizing high-dimensional data in 2 or 3 dimensions? \n",
    "1. Principal Component Analysis (PCA) \n",
    "2. Linear Discriminant Analysis (LDA) \n",
    "3. t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "4. Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3149e7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**t-SNE (t-Distributed Stochastic Neighbor Embedding)** is a **non-linear** dimensionality reduction technique that is:\n",
    "\n",
    "* Specifically designed for **visualizing high-dimensional data** in **2 or 3 dimensions**.\n",
    "* It preserves **local structure** (i.e., similarity between nearby points), making it excellent for **clustering and visualization**.\n",
    "* Commonly used in **image recognition, NLP, and exploratory data analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are less suitable for visualization:\n",
    "\n",
    "* **1. PCA** – A **linear** technique that reduces dimensions while preserving **variance**, but it doesn't capture complex, non-linear relationships as well as t-SNE for visualization.\n",
    "* **2. LDA** – A supervised technique focused on **maximizing class separability**, not general visualization.\n",
    "* **4. Autoencoders** – Can reduce dimensions and be used for visualization, but require **neural network training** and are more complex than t-SNE for direct visualization tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Use t-SNE when your goal is to visualize patterns or clusters in high-dimensional data in 2D or 3D.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68a6b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7fffcb",
   "metadata": {},
   "source": [
    "## 10. In time series data, which imputation method is commonly used to fill in missing values? \n",
    "1. Mean Imputation \n",
    "2. Hot-Deck Imputation \n",
    "3. Last Observation Carried Forward (LOCF) \n",
    "4. K-Nearest Neighbors (KNN) Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18f35d",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Last Observation Carried Forward (LOCF)**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "In **time series data**, the **Last Observation Carried Forward (LOCF)** method is commonly used to fill in missing values by:\n",
    "\n",
    "* Replacing a missing value with the **last available (previous) observation**.\n",
    "* Preserving the **temporal order** and making minimal assumptions about future trends.\n",
    "\n",
    "This method works well when:\n",
    "\n",
    "* The variable is **stable or changes slowly over time**.\n",
    "* The missing values are **not too frequent**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are less suitable for time series:\n",
    "\n",
    "* **1. Mean Imputation** – Ignores temporal structure by using a **global average**, which can **distort trends** in time series.\n",
    "* **2. Hot-Deck Imputation** – Fills missing values using **similar records**, not suitable for sequential dependencies.\n",
    "* **4. K-Nearest Neighbors (KNN) Imputation** – Can work, but is **computationally intensive** and not always effective for **sequential data** like time series.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "For time series, **LOCF** is a simple and effective method to impute missing values while **maintaining the time-based nature** of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725827b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc4e4c3",
   "metadata": {},
   "source": [
    "## 11. What is the main advantage of using K-Nearest Neighbors (KNN) imputation for missing data? \n",
    "1. It always provides the most accurate results\n",
    "2. It can handle non-linear relationships between variables \n",
    "3. It is computationally inexpensive \n",
    "4. It is unaffected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7f7ad",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It can handle non-linear relationships between variables**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**K-Nearest Neighbors (KNN) imputation** works by:\n",
    "\n",
    "* Finding the **k most similar records** (neighbors) to the one with missing data (based on distance metrics like Euclidean distance).\n",
    "* Then **imputing** the missing value using the values from these neighbors (e.g., mean or mode of neighbors’ values).\n",
    "\n",
    "This method is powerful because:\n",
    "\n",
    "* It can **capture complex, non-linear relationships** between features that simpler methods like mean or median imputation cannot.\n",
    "* It adapts based on the **local structure** of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. It always provides the most accurate results** – No method is perfect; KNN works well in some contexts, but **not always the most accurate**, especially with large or noisy datasets.\n",
    "* **3. It is computationally inexpensive** – Actually, **KNN is computationally expensive**, especially on large datasets, since it calculates distances for each missing value.\n",
    "* **4. It is unaffected by outliers** – KNN **can be sensitive to outliers**, as they may influence the selection of neighbors and skew the imputation.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**KNN imputation’s main advantage** is its ability to model **non-linear dependencies**, making it more flexible than traditional imputation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672e249",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee251182",
   "metadata": {},
   "source": [
    "## 12. What is the risk of using the mean imputation method for missing data? \n",
    "1. It introduces bias by inflating variance \n",
    "2. It preserves the distribution of the data \n",
    "3. It can lead to underfitting \n",
    "4. It increases the sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38b8a5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It can lead to underfitting**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Mean imputation** fills in missing values with the **mean of the observed values** for that feature. While it's simple and fast, it comes with significant drawbacks:\n",
    "\n",
    "* It **reduces variability** in the data.\n",
    "* It can **mask relationships** between variables.\n",
    "* By making data artificially uniform, it may cause models to **underfit** (i.e., fail to capture the underlying patterns in the data).\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. It introduces bias by inflating variance** – Actually, mean imputation tends to **reduce variance**, not inflate it.\n",
    "* **2. It preserves the distribution of the data** – False. It **distorts the distribution**, especially in skewed or multimodal data.\n",
    "* **4. It increases the sample size** – Imputation doesn’t increase sample size; it **fills in missing values**, keeping the sample size the same.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Mean imputation can **oversimplify the data**, which may lead to **underfitting**, especially in models that rely on capturing subtle patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f929d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0222913",
   "metadata": {},
   "source": [
    "## 13. What is the main purpose of using interaction terms in feature engineering? \n",
    "1. To remove irrelevant features \n",
    "2. To capture non-linear relationships between features \n",
    "3. To scale features to a common range \n",
    "4. To reduce the dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925cfb7",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To capture non-linear relationships between features**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Interaction terms** in feature engineering are **combinations of two or more features** (e.g., $x_1 \\times x_2$) used to:\n",
    "\n",
    "* **Capture complex relationships** between variables that are **not modeled well** by linear terms alone.\n",
    "* Help models (especially **linear models**) account for **non-linear effects** by introducing new variables that reflect **combined influence**.\n",
    "\n",
    "This is especially useful in:\n",
    "\n",
    "* **Regression models**, where the effect of one variable might **depend on the level** of another variable.\n",
    "* **Polynomial regression** or when adding multiplicative or additive combinations of variables.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. To remove irrelevant features** – That’s done through **feature selection**, not interaction terms.\n",
    "* **3. To scale features to a common range** – That’s the job of **feature scaling** (e.g., Min-Max, StandardScaler).\n",
    "* **4. To reduce the dimensionality of the dataset** – Interaction terms actually **increase** dimensionality by adding new features.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **interaction terms** to **model relationships between features** that aren't captured by individual features alone — particularly **non-linear interactions** in otherwise linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f4057",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40ccea7",
   "metadata": {},
   "source": [
    "## 14. Which feature scaling method is most appropriate for data that follows a power law distribution? \n",
    "1. Min-Max Scaling \n",
    "2. Log Transformation \n",
    "3. Z-Score Scaling \n",
    "4. Robust Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69fe77",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Log Transformation**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "If your data follows a **power law distribution** (i.e., it is **heavily skewed** with **many small values and a few very large ones**), the most appropriate feature scaling method is:\n",
    "\n",
    "> **Log Transformation**\n",
    "\n",
    "Why?\n",
    "\n",
    "* It **compresses the range** of large values and **spreads out** small values.\n",
    "* It **reduces skewness**, making the data more **normally distributed** or closer to it.\n",
    "* This helps models **better interpret patterns**, especially when working with **non-linear or skewed data**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are not ideal:\n",
    "\n",
    "* **1. Min-Max Scaling** – Sensitive to outliers and does **not reduce skewness**.\n",
    "* **3. Z-Score Scaling (Standardization)** – Assumes normal distribution; **not effective for skewed/power law data**.\n",
    "* **4. Robust Scaling** – Handles outliers well but **does not address skewness or power-law behavior** as effectively as log transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **log transformation** to handle **power-law distributed data**, especially before applying models that assume more symmetric or linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb844d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14d17d5",
   "metadata": {},
   "source": [
    "## 15. What is the main advantage of using polynomial features in regression models? \n",
    "1. They reduce model complexity \n",
    "2. They can model non-linear relationships \n",
    "3. They improve model interpretability \n",
    "4. They eliminate the need for feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21738a6",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. They can model non-linear relationships**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Polynomial features** are used in regression models to:\n",
    "\n",
    "* **Capture non-linear relationships** between the input features and the target variable by adding terms like $x^2, x^3, x_1x_2$, etc.\n",
    "* Allow **linear models** (like linear regression) to fit **non-linear data** by including these additional transformed features.\n",
    "\n",
    "For example, transforming a feature $x$ into $[x, x^2, x^3]$ allows a linear regression model to fit **curved trends** in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. They reduce model complexity** – False. Polynomial features actually **increase** complexity by adding more terms.\n",
    "* **3. They improve model interpretability** – Not necessarily. **More complex models** with higher-degree polynomials are often **harder to interpret**.\n",
    "* **4. They eliminate the need for feature scaling** – Incorrect. In fact, polynomial features can **magnify scale differences**, making **feature scaling even more important**.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **polynomial features** to help regression models **learn non-linear patterns** in data without switching to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0331af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00da833e",
   "metadata": {},
   "source": [
    "## 16. When is it appropriate to use frequency encoding for categorical variables? \n",
    "1. When the categorical variable has no inherent order \n",
    "2. When the categorical variables has a very high cardinality \n",
    "3. When the categorical variables is ordinal \n",
    "4. When the categorical variable is binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa3408",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. When the categorical variable has a very high cardinality**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Frequency encoding** replaces each category in a categorical variable with the **number of times it appears** (its frequency) in the dataset. This method is particularly useful:\n",
    "\n",
    "* When dealing with **high-cardinality categorical variables** (i.e., those with many unique values).\n",
    "* It helps **avoid the curse of dimensionality** caused by one-hot encoding or other sparse techniques.\n",
    "* It is **simple, fast, and memory-efficient**, especially for tree-based models like decision trees or random forests.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are less appropriate:\n",
    "\n",
    "* **1. When the categorical variable has no inherent order** – Frequency encoding doesn’t impose order, but **this isn’t its main use case**.\n",
    "* **3. When the categorical variable is ordinal** – Ordinal encoding (assigning meaningful ranks) is usually more appropriate.\n",
    "* **4. When the categorical variable is binary** – Binary variables are already encoded as 0/1 (or can easily be), so frequency encoding is unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **frequency encoding** when you're working with **high-cardinality categorical features** and want to avoid creating too many columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b506d02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e10f3f",
   "metadata": {},
   "source": [
    "## 17. What is the impact of using log transformation on skewed data? \n",
    "1. It increases the skewness of the data \n",
    "2. It reduces the skewness, making the data more normally distributed \n",
    "3. It increases the range of the data \n",
    "4. It has no effect on the distribution of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90977ffd",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. It reduces the skewness, making the data more normally distributed**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Log transformation** is commonly used to handle **right-skewed (positively skewed)** data. It works by:\n",
    "\n",
    "* **Compressing large values** more than small ones\n",
    "* **Reducing the influence of extreme values (outliers)**\n",
    "* Making the distribution **closer to normal (bell-shaped)**, which many statistical and machine learning models assume\n",
    "\n",
    "This helps improve:\n",
    "\n",
    "* **Model performance**\n",
    "* **Interpretability**\n",
    "* **Symmetry in data visualization**\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. It increases the skewness of the data** – Opposite of what log transformation does.\n",
    "* **3. It increases the range of the data** – It **compresses** the range, especially for large values.\n",
    "* **4. It has no effect on the distribution of the data** – It **does affect** the distribution, especially when the data is skewed.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **log transformation** to **reduce skewness** and help models that perform better with **normally distributed inputs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6916d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f862d86c",
   "metadata": {},
   "source": [
    "## 18. Which technique should be used to handle missing data when the missingness is related to the value itself (MNAR)? \n",
    "1. Mean Imputation \n",
    "2. Complete Case Analysis \n",
    "3. Multiple Imputation with Missingness Indicators \n",
    "4. Listwise Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabdc54",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Multiple Imputation with Missingness Indicators**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "When data is **Missing Not At Random (MNAR)** — meaning the **likelihood of a value being missing is related to the value itself** (e.g., people with high income choosing not to report it) — standard imputation methods like mean or deletion can introduce bias.\n",
    "\n",
    "**Multiple Imputation with Missingness Indicators** helps by:\n",
    "\n",
    "* **Creating a binary indicator variable** that marks whether a value is missing.\n",
    "* **Using multiple imputation** to generate several plausible values for the missing data, incorporating uncertainty.\n",
    "* **Preserving the structure and possible dependence** between missingness and the variable itself.\n",
    "\n",
    "This approach improves model performance and **reduces bias** when missingness contains valuable information.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect for MNAR:\n",
    "\n",
    "* **1. Mean Imputation** – Assumes data is Missing Completely At Random (MCAR); **biases results under MNAR**.\n",
    "* **2. Complete Case Analysis** – Deletes rows with missing data; this can **introduce significant bias and data loss** under MNAR.\n",
    "* **4. Listwise Deletion** – Same as above; **not suitable** when missingness is informative.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Use **Multiple Imputation with Missingness Indicators** for MNAR situations to **account for the missingness mechanism and reduce bias**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5daef2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8862ef",
   "metadata": {},
   "source": [
    "## 19. How does covariance differ from correlation in terms of scale and interpretability? \n",
    "1. Covariance is standardized and easier to interpret, while correlation is dependent on the units of the variables \n",
    "2. Covariance is dependent on the units of the variables, making it harder to interpret, while correlation is standardized and easier to compare across datasets \n",
    "3. Covariance provides both th e direction and strength of the relationships, while correlation only indicates the direction \n",
    "4. Covariance is not sensitive to scale, while correlation is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bd6da",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Covariance is dependent on the units of the variables, making it harder to interpret, while correlation is standardized and easier to compare across datasets**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "* **Covariance** measures how two variables vary **together**, but:\n",
    "\n",
    "  * It is **not standardized**, meaning its value is affected by the **units and scale** of the variables.\n",
    "  * For example, changing the unit of one variable (e.g., from meters to centimeters) changes the covariance.\n",
    "\n",
    "* **Correlation** (typically Pearson correlation):\n",
    "\n",
    "  * Is a **standardized version** of covariance.\n",
    "  * Ranges between **-1 and +1**, making it **unit-free** and **easy to interpret** across different variables and datasets.\n",
    "  * Shows both the **strength and direction** of the linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Covariance is standardized...** – Incorrect. Covariance is **not standardized**, correlation is.\n",
    "* **3. Covariance provides both direction and strength...** – Covariance does indicate **direction**, but not a standardized **strength** like correlation.\n",
    "* **4. Covariance is not sensitive to scale...** – False. Covariance **is sensitive** to the **scale and units** of variables.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Covariance is **scale-dependent** and less interpretable, while correlation is **scale-invariant** and easier to understand and compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c758975",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae2acbdf",
   "metadata": {},
   "source": [
    "## 20. What does a Pearson Correlation Coefficient of -0.85 indicate about the relationship between two variables? \n",
    "1. A weak positive linear relationship \n",
    "2. A strong positive linear relationship \n",
    "3. a weak negative linear relationship \n",
    "4. A strong negative linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b70bf",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. A strong negative linear relationship**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "The **Pearson Correlation Coefficient** (r) measures the **strength and direction** of the **linear relationship** between two variables. It ranges from **-1 to +1**:\n",
    "\n",
    "* **+1** → Perfect **positive** linear relationship\n",
    "* **0** → **No** linear relationship\n",
    "* **–1** → Perfect **negative** linear relationship\n",
    "\n",
    "So:\n",
    "\n",
    "* An **r = –0.85** means:\n",
    "\n",
    "  * The two variables are **strongly correlated**\n",
    "  * As one **increases**, the other **tends to decrease**\n",
    "  * The relationship is **linear and negative**\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. A weak positive linear relationship** → Would be a small **positive** r (e.g., +0.2)\n",
    "* **2. A strong positive linear relationship** → Would be r close to **+1**\n",
    "* **3. A weak negative linear relationship** → Would be r close to **–0.2**\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "A Pearson correlation of **–0.85** = **strong, negative, linear** relationship between two variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae347230",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb244cdd",
   "metadata": {},
   "source": [
    "## 21. Why is feature engineering considered a crucial step in the machine learning pipeline? \n",
    "1. It reduces the need for data preprocessing \n",
    "2. It simplifies the data visualization process \n",
    "3. It improves model performance by capturing underlying patterns \n",
    "4. It guarantees the elimination of all data biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d5439",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. It improves model performance by capturing underlying patterns**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Feature engineering** is the process of creating, transforming, or selecting features (input variables) to help machine learning models:\n",
    "\n",
    "* **Better understand the data**\n",
    "* **Capture meaningful patterns or relationships**\n",
    "* **Improve prediction accuracy and generalization**\n",
    "\n",
    "Good feature engineering can **make a simple model perform exceptionally well**, while poor feature engineering can limit even the most complex algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. It reduces the need for data preprocessing** – False. Feature engineering **builds on top of** proper data preprocessing (e.g., handling missing values, scaling).\n",
    "* **2. It simplifies the data visualization process** – Not its main purpose; visualization is more about **exploration**, not modeling.\n",
    "* **4. It guarantees the elimination of all data biases** – No technique can **guarantee** this; biases must be carefully identified and addressed separately.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Feature engineering is crucial** because it **enhances model performance** by revealing deeper, more useful structures in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7c8e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "811cfb49",
   "metadata": {},
   "source": [
    "## 22. Which technique is NOT typically used for handling imbalanced data? \n",
    "1. SMOTE \n",
    "2. Random Oversampling \n",
    "3. Tomek Links \n",
    "4. Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a474e",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**4. Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)** is a **classification algorithm and dimensionality reduction technique**, **not a method for handling imbalanced data**.\n",
    "\n",
    "It works by:\n",
    "\n",
    "* Finding a linear combination of features that best separates the classes.\n",
    "* It's **sensitive to class imbalance**, but not designed to fix it.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ The other techniques are commonly used to handle imbalanced datasets:\n",
    "\n",
    "* **1. SMOTE (Synthetic Minority Over-sampling Technique):**\n",
    "  Creates synthetic samples for the minority class to balance the dataset.\n",
    "\n",
    "* **2. Random Oversampling:**\n",
    "  Randomly duplicates instances of the minority class.\n",
    "\n",
    "* **3. Tomek Links:**\n",
    "  A **data cleaning method** that removes overlapping examples from the majority class to make class boundaries clearer.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**LDA is a modeling technique**, not a resampling or balancing method — hence **not typically used** for handling imbalanced data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85e003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeeda70d",
   "metadata": {},
   "source": [
    "## 23. Which imputation method involves replacing missing values with the mean or median of the non-missing data? \n",
    "1. Last Observation carried Forward (LOCF) \n",
    "2. K-Nearest Neighbors (KNN) Imputation \n",
    "3. Simple Imputation \n",
    "4. Multiple Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023dc34",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Simple Imputation**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Simple Imputation** is the process of filling in missing values using a **single statistic** such as:\n",
    "\n",
    "* **Mean** (for numerical data)\n",
    "* **Median** (for skewed numerical data)\n",
    "* **Mode** (for categorical data)\n",
    "\n",
    "It is:\n",
    "\n",
    "* **Easy to implement**\n",
    "* **Fast and efficient**\n",
    "* Often used as a **baseline method** before trying more advanced techniques\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Last Observation Carried Forward (LOCF)** – Fills missing values using the **last observed value**, mainly used in **time series**.\n",
    "* **2. K-Nearest Neighbors (KNN) Imputation** – Fills missing values based on the values of the **nearest neighbors**.\n",
    "* **4. Multiple Imputation** – Involves creating **multiple versions** of the dataset by imputing values multiple times to reflect uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Simple imputation** uses the **mean, median, or mode** to fill missing data and is the **most basic imputation technique**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb92086",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "871d6c78",
   "metadata": {},
   "source": [
    "## 24. In the context of missing data, what does MCAR stand for? \n",
    "1. Missing Categorical at Random \n",
    "2. Missing Completely at Random \n",
    "3. Missing Conditionally at Random \n",
    "4. Missing Concurrently at Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee5d04",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. Missing Completely at Random**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**MCAR (Missing Completely at Random)** means that the **probability of a data point being missing is entirely unrelated** to:\n",
    "\n",
    "* The **value of the variable itself**\n",
    "* **Any other variables** in the dataset\n",
    "\n",
    "In other words, the missing data is **random and unbiased**. This is the **best-case scenario** for handling missing data because:\n",
    "\n",
    "* Analyses remain **unbiased** even if you **delete** or **impute** the missing values.\n",
    "* Techniques like **listwise deletion** or **mean imputation** are relatively safe under MCAR.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Missing Categorical at Random** – Not a valid term.\n",
    "* **3. Missing Conditionally at Random** – Possibly a confusion with **MAR (Missing At Random)**, which is different.\n",
    "* **4. Missing Concurrently at Random** – Not a recognized concept in missing data theory.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**MCAR = Missing Completely at Random** — no pattern or relationship in the missingness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4a48c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3d6030",
   "metadata": {},
   "source": [
    "## 25. What is the purpose of Principal Component Analysis (PCA) in feature extraction? \n",
    "1. To create new features by combining original features \n",
    "2. To reduce dimensionality by selecting the most important features \n",
    "3. To normalize the data \n",
    "4. To remove outliers from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea5b54",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**1. To create new features by combining original features**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a **feature extraction** and **dimensionality reduction** technique that:\n",
    "\n",
    "* **Creates new features (principal components)** by combining original features through linear transformations.\n",
    "* These components capture the **maximum variance** in the data with fewer dimensions.\n",
    "* The new features are **uncorrelated** and ordered by the amount of variance they capture.\n",
    "\n",
    "So PCA doesn't just select existing features — it **creates new ones** based on combinations of the original variables.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **2. To reduce dimensionality by selecting the most important features** – That's **feature selection**, not PCA. PCA creates **new features**, not select existing ones.\n",
    "* **3. To normalize the data** – PCA requires normalization **before** it’s applied, but **it doesn't perform normalization** itself.\n",
    "* **4. To remove outliers from the dataset** – PCA does **not remove outliers**; in fact, it can be **sensitive to them**.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**PCA is used to create new, compact features** that preserve most of the data’s variability, helping reduce dimensionality while retaining essential information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b5b09",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5013763",
   "metadata": {},
   "source": [
    "## 26. Which feature selection method uses Lasso regularization to select features? \n",
    "1. Filter methods \n",
    "2. Wrapper methods \n",
    "3. Embedded methods \n",
    "4. Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1f4ff",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Embedded methods**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Embedded methods** perform feature selection **during the model training process**. A key example of this is **Lasso regularization** (Least Absolute Shrinkage and Selection Operator), which:\n",
    "\n",
    "* Adds an **L1 penalty** to the loss function.\n",
    "* **Shrinks some coefficients to exactly zero**, effectively **selecting features** by eliminating the less important ones.\n",
    "* Helps in both **regularization** and **automatic feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Filter methods** – Use statistical techniques (like correlation or chi-square tests) **independently of any model**.\n",
    "* **2. Wrapper methods** – Use a model to evaluate different **combinations of features** (e.g., forward/backward selection) but do **not use regularization**.\n",
    "* **4. Recursive Feature Elimination (RFE)** – A **wrapper method** that recursively removes the least important features based on model performance, but **not based on Lasso**.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Lasso-based feature selection** is an example of an **embedded method** — it’s built into the model training process itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0612741",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "056ce78b",
   "metadata": {},
   "source": [
    "## 27. What is the primary goal of feature transformation? \n",
    "1. To eliminate irrelevant features \n",
    "2. To change the distribution or scale of features \n",
    "3. To create interaction terms between features \n",
    "4. To combine features into a single feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3982de5",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**2. To change the distribution or scale of features**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Feature transformation** involves applying mathematical functions to your features to:\n",
    "\n",
    "* **Adjust the scale** (e.g., using Standardization or Min-Max Scaling),\n",
    "* **Normalize distributions** (e.g., using Log, Square Root, or Box-Cox transformations),\n",
    "* **Handle skewed data** to make it more suitable for modeling,\n",
    "* Improve the **performance and convergence** of machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. To eliminate irrelevant features** → That’s **feature selection**, not transformation.\n",
    "* **3. To create interaction terms between features** → That’s a part of **feature engineering**, specifically interaction creation.\n",
    "* **4. To combine features into a single feature** → That can be done via **feature extraction** (like PCA), not the primary goal of transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "The main purpose of **feature transformation** is to **change the distribution or scale** of features so that they are better suited for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f1857",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e528f6",
   "metadata": {},
   "source": [
    "## 28. Which of the following is an example of feature creation? \n",
    "1. Normalization \n",
    "2. Standardization \n",
    "3. Creating BMI from height and weight \n",
    "4. Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30806a",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "\n",
    "**3. Creating BMI from height and weight**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Feature creation** (a part of feature engineering) involves **creating new features** from existing ones to help the model better understand underlying patterns.\n",
    "\n",
    "* **Example:** Calculating **BMI (Body Mass Index)** from **height and weight** creates a new, more meaningful feature from two raw features.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Normalization** → This is a **scaling technique**, not feature creation.\n",
    "* **2. Standardization** → Another **scaling technique**, not creating new features.\n",
    "* **4. Log transformation** → This is a **feature transformation**, used to deal with skewness or to stabilize variance, but it doesn't create a new feature from multiple inputs.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "Creating **BMI from height and weight** is a classic example of **feature creation**—combining existing features into a new one that may capture more useful information for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb6fbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0112da",
   "metadata": {},
   "source": [
    "## 29. When dealing with imbalanced data, what does the SMOTE technique do? \n",
    "1. Removes majority class samples \n",
    "2. Adds noise to the minority class \n",
    "3. Creates synthetic samples of the minority class \n",
    "4. Adjusts class weights during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97977317",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. Creates synthetic samples of the minority class**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**SMOTE** stands for **Synthetic Minority Over-sampling Technique**.\n",
    "It is a popular **oversampling** method used to handle **imbalanced datasets**.\n",
    "\n",
    "* SMOTE works by **creating new synthetic samples** for the **minority class** rather than just duplicating existing ones.\n",
    "* It does this by **interpolating** between existing minority class examples and their nearest neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **1. Removes majority class samples** → That’s **undersampling**, not SMOTE.\n",
    "* **2. Adds noise to the minority class** → SMOTE does not add noise; it creates synthetic, structured samples.\n",
    "* **4. Adjusts class weights during model training** → This is another technique (like using `class_weight='balanced'` in models), not what SMOTE does.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "**SMOTE = Synthetic generation of minority class samples** to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dcf69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a918e0",
   "metadata": {},
   "source": [
    "## 30. What is the main advantage of using an Isolation Forest for outlier detection? \n",
    "1. It is computationally less expensive than other methods \n",
    "2. It requires no tuning or parameters\n",
    "3. It isolates outliers more quickly than normal points \n",
    "4. It works better with categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7dae2",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. It isolates outliers more quickly than normal points**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Isolation Forest** is an ensemble-based outlier detection algorithm that works by **randomly selecting a feature and then randomly selecting a split value** between the maximum and minimum values of the selected feature.\n",
    "\n",
    "* **Outliers** are more likely to be **isolated (split apart)** early in the process because they are **few and different**.\n",
    "* Hence, they generally have **shorter path lengths** in the tree structure, allowing the model to detect them quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why other options are incorrect:\n",
    "\n",
    "* **1. It is computationally less expensive than other methods** → While efficient, this is not the primary reason it's used.\n",
    "* **2. It requires no tuning or parameters** → It does have parameters like `n_estimators`, `max_samples`, etc., though it’s relatively simple to use.\n",
    "* **4. It works better with categorical data** → It is **not well-suited for categorical data**; it works best with **numerical data**.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "Isolation Forest is effective because **outliers are easier to isolate**, which the algorithm does faster than for normal data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a406294",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f20652d1",
   "metadata": {},
   "source": [
    "## 31. Which method of feature selection considers all possible subsets of features to find the best one? \n",
    "1. Forward Selection \n",
    "2. Backward Elimination \n",
    "3. Recursive Feature Elimination \n",
    "4. Exhaustive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedcdc28",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**4. Exhaustive Search**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Exhaustive Search** (also called **best subset selection**) evaluates **all possible combinations of features** to determine which subset performs best according to a specific evaluation metric (like accuracy, AIC, BIC, etc.).\n",
    "\n",
    "* It is the most **comprehensive** feature selection method.\n",
    "* However, it is also the **most computationally expensive**, especially when dealing with a large number of features.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why other options are incorrect:\n",
    "\n",
    "* **1. Forward Selection** → Starts with no features and **adds one at a time** based on performance.\n",
    "* **2. Backward Elimination** → Starts with all features and **removes one at a time** based on least contribution.\n",
    "* **3. Recursive Feature Elimination (RFE)** → Recursively removes the **least important features** using model-based rankings.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "**Exhaustive Search** tries **all subsets**, making it thorough but slow for high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134f523",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b166b35f",
   "metadata": {},
   "source": [
    "## 32. How does the Curse of Dimensionality affect feature engineering? \n",
    "1. It reduces the accuracy of machine learning models \n",
    "2. It increases the interpretability of models \n",
    "3. It simplifies feature selection \n",
    "4. It makes it easier to detect outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed24a17",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**1. It reduces the accuracy of machine learning models**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "The **Curse of Dimensionality** refers to various problems that arise when working with data in **high-dimensional spaces**:\n",
    "\n",
    "* **Sparse data**: As dimensions increase, data becomes sparse, making it difficult for models to generalize.\n",
    "* **Increased computation**: More features = more resources needed to process data.\n",
    "* **Overfitting**: Models may learn noise instead of patterns due to too many irrelevant features.\n",
    "* **Distance measures become less meaningful**, affecting algorithms like KNN or clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why other options are incorrect:\n",
    "\n",
    "* **2. Increases interpretability** → More features usually make models **harder** to interpret.\n",
    "* **3. Simplifies feature selection** → It actually makes feature selection **more complex and crucial**.\n",
    "* **4. Makes it easier to detect outliers** → High dimensions can **hide** outliers due to sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "The Curse of Dimensionality **negatively impacts model performance** by making the data more sparse and the models prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f259d5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14f2a67",
   "metadata": {},
   "source": [
    "## 33. What does the term \"data leakage\" refer to in the context of feature engineering? \n",
    "1. The process of accidentally including future information in the training data \n",
    "2. The loss of data due to improper handling \n",
    "3. The sharing of sensitive data without authorization \n",
    "4. The mislabeling of data during preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33114107",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**1. The process of accidentally including future information in the training data**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Data leakage** occurs when **information from outside the training dataset** — especially from the **future or test data** — is used to train the model. This leads to **unrealistically high performance during training**, but **poor generalization** on unseen data.\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Including a target variable or a future-derived feature in your training features.\n",
    "* Using data that would not be available at prediction time.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the other options are incorrect:\n",
    "\n",
    "* **2. Loss of data due to improper handling** → This is more about data corruption, not leakage.\n",
    "* **3. Sharing of sensitive data** → This is a **security/privacy breach**, not data leakage in ML context.\n",
    "* **4. Mislabeling of data** → That's a **labeling error**, not data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "**Data leakage** can mislead your model to perform well during training but fail in production. It’s one of the most critical issues to avoid in feature engineering and model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68645aba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69f7906e",
   "metadata": {},
   "source": [
    "## 34. How can high-cardinality categorical variables negatively impact a machine learning model? \n",
    "1. They increase the interpretability of the model \n",
    "2. They reduce the dimensionality of the dataset \n",
    "3. They can lead to overfitting due to the large number of unique values \n",
    "4. They always improve the model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e97c4",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. They can lead to overfitting due to the large number of unique values**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**High-cardinality categorical variables** (e.g., \"zip code\", \"user ID\") have **many unique values**. When encoded (e.g., using one-hot encoding), they can:\n",
    "\n",
    "* **Greatly increase the number of features**, leading to **high dimensionality**\n",
    "* Cause the model to **memorize** the data instead of generalizing → **overfitting**\n",
    "* Be **sparse** and less informative for prediction\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Incorrect options:\n",
    "\n",
    "* **1. Increase interpretability** → No, they make models harder to interpret.\n",
    "* **2. Reduce dimensionality** → They do the opposite; they increase it.\n",
    "* **4. Always improve accuracy** → Not true; they often **hurt performance** without proper handling.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "High-cardinality categorical variables can make models **complex, overfit-prone**, and **computationally inefficient**. Techniques like **target encoding, frequency encoding, or embedding** can help handle them effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4f696",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a460d35",
   "metadata": {},
   "source": [
    "## 35. Which of the following is a disadvantage of using recursive feature elimination (RFE)? \n",
    "1. It cannot handle categorical features \n",
    "2. It is computationally expensive for large datasets \n",
    "3. It ignores feature importance \n",
    "4. It does not provide a ranked list of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ee76b",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. It is computationally expensive for large datasets**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Recursive Feature Elimination (RFE)** works by:\n",
    "\n",
    "* Fitting the model multiple times\n",
    "* Recursively removing the **least important features** based on model performance\n",
    "  This makes it **computationally expensive**, especially for:\n",
    "* **Large datasets**\n",
    "* **High-dimensional data**\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Incorrect options:\n",
    "\n",
    "* **1. It cannot handle categorical features** → Not true. RFE can work with categorical features if they are properly encoded (e.g., one-hot, label encoding).\n",
    "* **3. It ignores feature importance** → False. RFE is **based on feature importance** (e.g., weights from linear models, feature importance from trees).\n",
    "* **4. It does not provide a ranked list of features** → Incorrect. RFE **does** provide a ranking of features during the elimination process.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "RFE is powerful but can be **slow and resource-intensive** for large datasets. Use it when you have **enough computation power** or combine it with other methods for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850703b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3e2e0d",
   "metadata": {},
   "source": [
    "## 36. How does the Ridge regularization method help in feature selection? \n",
    "1. It removes irrelevant features by setting coeficients to zero \n",
    "2. It shrinks coefficients less important features towards zero without eliminating them \n",
    "3. It increases the coefficients of important features \n",
    "4. It selects features based on correlation with the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7997d46",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. It shrinks coefficients of less important features towards zero without eliminating them**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Ridge Regularization** (also known as **L2 regularization**) works by:\n",
    "\n",
    "* Adding a **penalty** equal to the **square of the magnitude** of coefficients to the loss function.\n",
    "* This **shrinks the coefficients** of less important features **closer to zero** but **does not set them exactly to zero**.\n",
    "* Therefore, Ridge **does not perform feature selection** in the strict sense (i.e., it doesn’t eliminate features), but it **helps reduce overfitting** and handles multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the others are incorrect:\n",
    "\n",
    "* **1. It removes irrelevant features by setting coefficients to zero**\n",
    "  → This is true for **Lasso (L1 regularization)**, not Ridge.\n",
    "\n",
    "* **3. It increases the coefficients of important features**\n",
    "  → No, Ridge typically **shrinks all coefficients**; it doesn’t increase any.\n",
    "\n",
    "* **4. It selects features based on correlation with the target variable**\n",
    "  → This sounds like a filter method, not Ridge. Ridge is a **model-based regularization method**, not a correlation-based selection method.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **Ridge (L2):** Shrinks all coefficients but keeps all features.\n",
    "* **Lasso (L1):** Can shrink some coefficients to **zero**, effectively doing **feature selection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a531942",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c68415b1",
   "metadata": {},
   "source": [
    "## 37. Why is it important to ensure that feature engineering is applied consistently during training and testing phases?\n",
    "1. To improve model interpretability \n",
    "2. To avoid data leakage \n",
    "3. To increase the dimensionality of the dataset \n",
    "4. To ensure the model is unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35897c27",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. To avoid data leakage**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "Applying **feature engineering consistently** during both the **training** and **testing** phases is crucial to:\n",
    "\n",
    "* Ensure that the model sees **data in the same format** during both phases.\n",
    "* **Avoid data leakage**, which happens when information from outside the training dataset is used to create the model — leading to **overly optimistic performance** and **poor generalization** to unseen data.\n",
    "\n",
    "For example:\n",
    "\n",
    "* If you scale features (like normalization or standardization) using the **mean and std of the entire dataset** (including test data), the model may inadvertently learn information from the test set — this is **data leakage**.\n",
    "* The correct way is to compute transformation parameters **only from the training set** and **apply them** to both training and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the others are incorrect:\n",
    "\n",
    "* **1. To improve model interpretability**\n",
    "  → Consistency doesn’t directly impact interpretability — that's more about feature clarity and model type.\n",
    "\n",
    "* **3. To increase the dimensionality of the dataset**\n",
    "  → We usually **want to reduce** or control dimensionality — not increase it unnecessarily.\n",
    "\n",
    "* **4. To ensure the model is unbiased**\n",
    "  → While consistency **may** help reduce bias, the primary goal is to **prevent leakage and ensure valid evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "> **Consistent feature engineering ensures that your model is evaluated fairly and generalizes well — by preventing test data from “leaking” into training.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1ffe8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a60cf73",
   "metadata": {},
   "source": [
    "## 38. Which of the following best describes the main purpose of Principal Component Analysis (PCA)? \n",
    "1. To create new features that are correlated with the original data \n",
    "2. To transform the data into a set of uncorrelated variables while retaining most of the data's variance \n",
    "3. To reduce the sample size of the dataset \n",
    "4. To ensure all features have a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff49a2",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**2. To transform the data into a set of uncorrelated variables while retaining most of the data's variance**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a **dimensionality reduction** technique that:\n",
    "\n",
    "* **Transforms** the original features into a new set of features called **principal components**\n",
    "* These components are:\n",
    "\n",
    "  * **Uncorrelated** (orthogonal to each other)\n",
    "  * Ordered by the amount of **variance** they capture from the original data\n",
    "* The goal is to **reduce dimensionality** while **preserving as much variance as possible**\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the others are incorrect:\n",
    "\n",
    "* **1. To create new features that are correlated with the original data**\n",
    "  → PCA creates **uncorrelated** components; they may not be easily interpretable in terms of original features.\n",
    "\n",
    "* **3. To reduce the sample size of the dataset**\n",
    "  → PCA reduces the **number of features**, not the number of **samples**.\n",
    "\n",
    "* **4. To ensure all features have a mean of 0 and a standard deviation of 1**\n",
    "  → This describes **standardization**, which is often done **before** PCA, but it’s **not** PCA's main purpose.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "> **PCA helps simplify complex datasets by converting them into a set of uncorrelated variables that still capture the majority of the original variance.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3fe32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f430203",
   "metadata": {},
   "source": [
    "## 39. When would it be more appropriate to use Spearman's Rank Correlation over Pearson's Correlation? \n",
    "1. When the data is continuous and normally distributed \n",
    "2. When the relationship between variables is expected to be linear \n",
    "3. When the data is ordinal or not normally distributed, or when there are outliers \n",
    "4. When you need to measure the strength of a linear relationship between two continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20e724",
   "metadata": {},
   "source": [
    "The correct answer is:\n",
    "**3. When the data is ordinal or not normally distributed, or when there are outliers**\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Explanation:\n",
    "\n",
    "**Spearman's Rank Correlation** is a **non-parametric** measure of correlation. It is best used when:\n",
    "\n",
    "* The data is **ordinal** (ranked), or\n",
    "* The variables have a **monotonic** relationship (not necessarily linear), or\n",
    "* The data is **not normally distributed**, or\n",
    "* There are **outliers** that could distort Pearson’s correlation\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Why the others are incorrect:\n",
    "\n",
    "* **1. When the data is continuous and normally distributed**\n",
    "  → Use **Pearson’s correlation** for this case.\n",
    "\n",
    "* **2. When the relationship between variables is expected to be linear**\n",
    "  → Again, **Pearson’s correlation** is more appropriate.\n",
    "\n",
    "* **4. When you need to measure the strength of a linear relationship between two continuous variables**\n",
    "  → This directly describes **Pearson’s correlation**, not Spearman’s.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "> **Use Spearman’s Rank Correlation when data is ordinal, not normally distributed, or contains outliers, and you're interested in monotonic relationships.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed687fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e3027",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9fa9a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7fcef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
