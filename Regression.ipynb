{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa7d9c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee5fd39",
   "metadata": {},
   "source": [
    "# QUIZ : REGRESSION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9b7de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "993d3091",
   "metadata": {},
   "source": [
    "## Q1. What is Primary goal of linear regression? \n",
    "1. Minimize the residual sum of squares between observed and predicted values \n",
    "2. Maximize the number of predictors \n",
    "3. Find the correlation between variables \n",
    "4. Reduce the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe1f51",
   "metadata": {},
   "source": [
    "The correct answer is: **1. Minimize the residual sum of squares between observed and predicted values** ✅\n",
    "\n",
    "Linear regression’s main goal is to find the best-fit line that **minimizes the sum of squared differences** (residuals) between the actual values and the values predicted by the model. This is called the **Ordinary Least Squares (OLS)** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e934f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e25b935",
   "metadata": {},
   "source": [
    "## Q2. What does the Elastic Net parameter alpha control? \n",
    "1. The weight of L1 and L2 regularization \n",
    "2. The learning rate \n",
    "3. The number of iterations \n",
    "4. The size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d84771",
   "metadata": {},
   "source": [
    "The correct answer is: **1. The weight of L1 and L2 regularization** ✅\n",
    "\n",
    "In **Elastic Net regression**, the parameter **alpha** controls how much **L1 regularization (Lasso)** and **L2 regularization (Ridge)** are mixed.\n",
    "\n",
    "* **Alpha = 1** → Pure Lasso (L1)\n",
    "* **Alpha = 0** → Pure Ridge (L2)\n",
    "* **Between 0 and 1** → A combination of both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104f4cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245494ed",
   "metadata": {},
   "source": [
    "## Q3. What is the Elastic Net method? \n",
    "1. A combination of L1 and L2 regularization \n",
    "2. A type of Ridge regression \n",
    "3. A method to maximize the cost function \n",
    "4. A method for unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cac6c5",
   "metadata": {},
   "source": [
    "The correct answer is: **1. A combination of L1 and L2 regularization** ✅\n",
    "\n",
    "**Elastic Net** combines:\n",
    "\n",
    "* **L1 regularization (Lasso)** → helps with feature selection (drives some coefficients to zero)\n",
    "* **L2 regularization (Ridge)** → helps with multicollinearity and stabilizing coefficients\n",
    "\n",
    "This hybrid approach is especially useful when there are **many correlated features**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf13a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21031b8c",
   "metadata": {},
   "source": [
    "## Q4. What is a key feature of Lasso Regression that distinguishes it from Ridge Regression? \n",
    "1. It increases the coefficients \n",
    "2. It can shrink some coefficients to zero, effectively performing feature selection \n",
    "3. It penalizes the square of the coefficients \n",
    "4. It uses both L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3cab0",
   "metadata": {},
   "source": [
    "The correct answer is: **2. It can shrink some coefficients to zero, effectively performing feature selection** ✅\n",
    "\n",
    "**Lasso Regression** uses **L1 regularization**, which can force some coefficients to become exactly zero — meaning those features are effectively removed from the model. This is the main distinction from **Ridge Regression**, which uses **L2 regularization** and only shrinks coefficients toward zero but never exactly to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50a1ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40267d93",
   "metadata": {},
   "source": [
    "## Q5. How is the Ridge parameter typically chosen? \n",
    "1. By trial and error \n",
    "2. Using cross-validation \n",
    "3. By setting it to zero \n",
    "4. Based on the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff81580",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Using cross-validation** ✅\n",
    "\n",
    "In **Ridge Regression**, the regularization strength parameter (**λ** or **alpha**) is usually chosen through **cross-validation**, where multiple values are tested, and the one giving the best model performance on validation data is selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2d7a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1beed1a",
   "metadata": {},
   "source": [
    "## Q6. What is the amin difference between Ridge Regression and OLS (Ordinary Least Squares)? \n",
    "1. Ridge Regression minimizes the sum of squared residuals \n",
    "2. Ridge Regression adds a penalty for large coefficients \n",
    "3. Ridge Regression maximizes the cost function \n",
    "4. Ridge Regression removes irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e147f",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Ridge Regression adds a penalty for large coefficients** ✅\n",
    "\n",
    "* **OLS**: Minimizes only the **sum of squared residuals**.\n",
    "* **Ridge Regression**: Minimizes the **sum of squared residuals + L2 penalty term** (which is the sum of squared coefficients).\n",
    "  This penalty discourages very large coefficient values, helping to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190086ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d55b380",
   "metadata": {},
   "source": [
    "## Q7. What is the main purpose of regularization in linear models? \n",
    "1. Increase the complexity of the model \n",
    "2. Prevent overfitting by penalizing large coefficients \n",
    "3. Improve the model's speed \n",
    "4. Ensure perfect accuracy on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd0578",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Prevent overfitting by penalizing large coefficients** ✅\n",
    "\n",
    "Regularization (like **L1**, **L2**, or **Elastic Net**) discourages overly complex models by adding a penalty term to the loss function, which shrinks large coefficients. This helps the model generalize better to **unseen data** instead of just memorizing the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b975cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674eea23",
   "metadata": {},
   "source": [
    "## Q8. Which metric is least sensitive to outliers? \n",
    "1. RMSE \n",
    "2. MSE \n",
    "3. MAE\n",
    "4. R Square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09c84",
   "metadata": {},
   "source": [
    "The correct answer is: **3. MAE (Mean Absolute Error)** ✅\n",
    "\n",
    "* **MSE** and **RMSE** square the errors, so large errors (outliers) have a much bigger impact.\n",
    "* **MAE** takes the absolute value of errors, giving equal weight to all errors and making it **less sensitive to outliers**.\n",
    "* **R²** isn’t a direct error metric and can still be affected by outliers through variance changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991cd2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b752967",
   "metadata": {},
   "source": [
    "## Q9. Which of the following is an assumption of linear regression? \n",
    "1. Homoscedasticity \n",
    "2. Heteroscedasticity \n",
    "3. Multicollinearity \n",
    "4. Non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fc2de",
   "metadata": {},
   "source": [
    "The correct answer is: **1. Homoscedasticity** ✅\n",
    "\n",
    "**Homoscedasticity** means that the variance of the residuals (errors) is constant across all levels of the independent variable(s).\n",
    "It’s a core assumption of linear regression, along with:\n",
    "\n",
    "* Linearity\n",
    "* Independence of errors\n",
    "* Normally distributed errors\n",
    "* No perfect multicollinearity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da83c0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede266e5",
   "metadata": {},
   "source": [
    "## Q10. What does the slope coefficient in simple linear regression represent? \n",
    "1. The intercept of the regression line \n",
    "2. The change in the dependent variable for a unit change in the independent variable \n",
    "3. The error term \n",
    "4. The mean of the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acebe4",
   "metadata": {},
   "source": [
    "The correct answer is: **2. The change in the dependent variable for a unit change in the independent variable** ✅\n",
    "\n",
    "In **simple linear regression**:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "$$\n",
    "\n",
    "* **$\\beta_1$** (slope) tells us how much **Y** changes when **X** increases by 1 unit, assuming all else stays constant.\n",
    "* **$\\beta_0$** is the intercept, **$\\epsilon$** is the error term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1aa2cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd62b948",
   "metadata": {},
   "source": [
    "## Q11. What is the objective of gradient descent? \n",
    "1. Maximize the cost function \n",
    "2. Find the local minima of the cost function \n",
    "3. Increase the learning rate \n",
    "4. Perform feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54fe34",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Find the local minima of the cost function** ✅\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to update model parameters (like weights in regression or neural networks) in order to **minimize the cost/loss function**.\n",
    "It works by moving step-by-step in the opposite direction of the gradient until it reaches the minimum point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d8320",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efa42fe8",
   "metadata": {},
   "source": [
    "## Q12. Which type of gradient descet uses the entire dataset to compute the gradient? \n",
    "1. Stochastic Gradient Descent \n",
    "2. Mini-batch Gradient Descent \n",
    "3. Batch Gradient Descent \n",
    "4. Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f621f74",
   "metadata": {},
   "source": [
    "The correct answer is: **3. Batch Gradient Descent** ✅\n",
    "\n",
    "* **Batch Gradient Descent** → Uses the **entire dataset** to compute the gradient before each update.\n",
    "* **Stochastic Gradient Descent (SGD)** → Uses **one sample at a time**.\n",
    "* **Mini-batch Gradient Descent** → Uses a **small subset of the dataset** for each update.\n",
    "* **Adam Optimizer** → An advanced optimization algorithm that adapts learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec98ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d615b5ad",
   "metadata": {},
   "source": [
    "## Q13. What is the primary advantage of Stochastic Gradient Descent (SGD) over Batch Gradient Descent? \n",
    "1. It converges faster \n",
    "2. It is more accurate \n",
    "3. It uses the entire dataset for updates \n",
    "4. It always finds the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33343bee",
   "metadata": {},
   "source": [
    "The correct answer is: **1. It converges faster** ✅\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** updates parameters **after each training example**, which:\n",
    "\n",
    "* Makes it faster for large datasets (more frequent updates).\n",
    "* Allows it to start improving the model without waiting to process the whole dataset.\n",
    "  However, it introduces more noise in updates, which can sometimes help escape local minima but also causes more fluctuations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc173080",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2226d45",
   "metadata": {},
   "source": [
    "## Q14. It always finds the global minimum \n",
    "1. One dependent variable and one independent variable \n",
    "2. Multiple dependent variables \n",
    "3. One dependent variable and multiple independent variables \n",
    "4. Multiple independent variables and dependent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4636a6c",
   "metadata": {},
   "source": [
    "It looks like your question is about identifying the correct definition for **multiple linear regression**.\n",
    "\n",
    "The correct answer is: **3. One dependent variable and multiple independent variables** ✅\n",
    "\n",
    "* **Simple Linear Regression** → 1 dependent variable, 1 independent variable\n",
    "* **Multiple Linear Regression** → 1 dependent variable, **2 or more** independent variables\n",
    "* **Multivariate Regression** → **Multiple dependent variables**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9e0ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd86a9c",
   "metadata": {},
   "source": [
    "## Q15. What does the p-value of a coefficient in multiple linear regression indicate? \n",
    "1. The strength of correlation between variables \n",
    "2. The probability that the coefficient is zero \n",
    "3. The effect size of the coefficient \n",
    "4. The residual error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f59d27",
   "metadata": {},
   "source": [
    "The correct answer is: **2. The probability that the coefficient is zero** ✅\n",
    "\n",
    "In multiple linear regression, the **p-value** for a coefficient tests the **null hypothesis** that the coefficient = 0 (no effect).\n",
    "\n",
    "* **Low p-value (< 0.05)** → Reject the null → The predictor is statistically significant.\n",
    "* **High p-value** → Fail to reject the null → The predictor may not be contributing significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425b879",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72569d8f",
   "metadata": {},
   "source": [
    "## Q16. Which techniques is commonly used for feature selection in multiple linear regression? \n",
    "1. Ridge Regression \n",
    "2. Recursive Feature Elimination (RFE) \n",
    "3. K-Means Clustering \n",
    "4. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee5ed4",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Recursive Feature Elimination (RFE)** ✅\n",
    "\n",
    "**Recursive Feature Elimination** works by:\n",
    "\n",
    "1. Fitting the model\n",
    "2. Ranking features by importance\n",
    "3. Removing the least important features\n",
    "4. Repeating the process until the desired number of features remains\n",
    "\n",
    "It’s a common method for **feature selection** in regression and classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10831120",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "221462ca",
   "metadata": {},
   "source": [
    "## Q17. What is the primary difference between linear and polynomial regression? \n",
    "1. The type of data used \n",
    "2. The degree of the polynomial used to fit the data \n",
    "3. The cost function \n",
    "4. The optimization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ae173",
   "metadata": {},
   "source": [
    "The correct answer is: **2. The degree of the polynomial used to fit the data** ✅\n",
    "\n",
    "* **Linear Regression** → Fits a straight line ($y = \\beta_0 + \\beta_1 x$)\n",
    "* **Polynomial Regression** → Fits a curve by including higher-degree terms ($x^2, x^3, \\dots$) in the model, while still being **linear in the coefficients**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee628c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec65149c",
   "metadata": {},
   "source": [
    "## Q18. When choosing the degree of polynomial in regression, increasing the degree too much can lead to \n",
    "1. Underfitting \n",
    "2. Overfitting \n",
    "3. Reducing bias \n",
    "4. Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ba0ec",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Overfitting** ✅\n",
    "\n",
    "When the **degree of the polynomial** is too high, the model can fit the training data **too closely**, capturing noise and fluctuations instead of the true underlying pattern.\n",
    "This results in:\n",
    "\n",
    "* Very low training error\n",
    "* Poor generalization to new/unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c9b61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac535484",
   "metadata": {},
   "source": [
    "## Q19. Which method can help identify the optimal degree of a polynomial model? \n",
    "1. Cross-validation \n",
    "2. Grid Search \n",
    "3. Both A and B \n",
    "4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdadb71",
   "metadata": {},
   "source": [
    "The correct answer is: **3. Both A and B** ✅\n",
    "\n",
    "* **Cross-validation** → Tests model performance on multiple splits of the data to find the best degree.\n",
    "* **Grid Search** → Systematically tries different degree values (and possibly other hyperparameters) to find the optimal one, often combined with cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dea50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837cddda",
   "metadata": {},
   "source": [
    "## Q20. What does R Squared represent in regression analysis?\n",
    "1. The total sum of squares \n",
    "2. The proportion of variance in the dependent variable explained by the independent variables \n",
    "3. The sum of squared residuals \n",
    "4. The model's error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc18d0",
   "metadata": {},
   "source": [
    "The correct answer is: **2. The proportion of variance in the dependent variable explained by the independent variables** ✅\n",
    "\n",
    "**R²** tells us how well the model fits the data:\n",
    "\n",
    "* **R² = 1** → Perfect fit (100% variance explained)\n",
    "* **R² = 0** → Model explains none of the variance\n",
    "* Higher R² means better fit, but a very high value can sometimes indicate overfitting if not validated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5205f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6d43d5",
   "metadata": {},
   "source": [
    "## Q 21. A major limitation of R Square is \n",
    "1. It decreases with the addition of predictors \n",
    "2. It increases regardless of the predictors' significance \n",
    "3. It is affected by multicollinearity \n",
    "4. It is always equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ce2d3",
   "metadata": {},
   "source": [
    "The correct answer is: **2. It increases regardless of the predictors' significance** ✅\n",
    "\n",
    "A key limitation of **R²** is that it **never decreases** when you add more predictors, even if those predictors have no real relationship with the dependent variable.\n",
    "That’s why **Adjusted R²** is often preferred — it penalizes adding irrelevant variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c84b5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba6d7c9",
   "metadata": {},
   "source": [
    "## Q22. How does Adjusted R Square improve upon R Sqaure? \n",
    "1. By penalizing the addition of irrelevant predictors \n",
    "2. By increasing the model complexity \n",
    "3. By ensuring the model is not underfitting \n",
    "4. By increasing with the number of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d8f4a",
   "metadata": {},
   "source": [
    "The correct answer is: **1. By penalizing the addition of irrelevant predictors** ✅\n",
    "\n",
    "**Adjusted R²** adjusts the R² value based on the number of predictors and the sample size.\n",
    "\n",
    "* If a new predictor doesn’t improve the model enough, **Adjusted R² will decrease**.\n",
    "* This makes it a better metric for comparing models with different numbers of predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8a264",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7481354",
   "metadata": {},
   "source": [
    "## Q23. Which metric is most sensitive to outliers? \n",
    "1. RMSE \n",
    "2. MSE \n",
    "3. MAE \n",
    "4. All are equally sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139136a",
   "metadata": {},
   "source": [
    "The correct answer is: **2. MSE (Mean Squared Error)** ✅\n",
    "\n",
    "Because **MSE** squares the errors, **large errors from outliers become disproportionately larger**, making MSE more sensitive to outliers than RMSE or MAE.\n",
    "\n",
    "* **RMSE** is also sensitive but slightly less so because it takes the square root at the end.\n",
    "* **MAE** is least sensitive since it uses absolute values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a42bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1ebcf3",
   "metadata": {},
   "source": [
    "## Q24. What does RMSE measure in regression models? \n",
    "1. The square root of the average squared differences between actual and predicted values \n",
    "2. The average squared differences between actual and predicted values \n",
    "3. The average absolute differences between actual and predicted values \n",
    "4. The average difference between actual and predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a435af",
   "metadata": {},
   "source": [
    "The correct answer is: **1. The square root of the average squared differences between actual and predicted values** ✅\n",
    "\n",
    "**RMSE (Root Mean Squared Error)** gives the standard deviation of prediction errors:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{\\sum (y_{pred} - y_{actual})^2}{n}}\n",
    "$$\n",
    "\n",
    "It’s in the **same units** as the dependent variable, making it easier to interpret compared to MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d167206",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7fc648",
   "metadata": {},
   "source": [
    "## Q25. When is MAE preferred over RMSE? \n",
    "1. When the data contains significant outliers \n",
    "2. When penalizing larger errors is important \n",
    "3. When interpreting squared errors is difficult \n",
    "4. Both A and C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca672c",
   "metadata": {},
   "source": [
    "The correct answer is: **4. Both A and C** ✅\n",
    "\n",
    "**MAE (Mean Absolute Error)** is preferred when:\n",
    "\n",
    "* **A** → The dataset has significant outliers (MAE is less sensitive to them).\n",
    "* **C** → You want a more intuitive interpretation without squaring errors — it directly represents the average absolute error in the same units as the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761081a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d77f6dc1",
   "metadata": {},
   "source": [
    "## Q26. Which of the following is a benefit of regularization? \n",
    "1. It reduces bias \n",
    "2. It prevents overfitting \n",
    "3. It increases the number of features \n",
    "4. It improves underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3c812",
   "metadata": {},
   "source": [
    "The correct answer is: **2. It prevents overfitting** ✅\n",
    "\n",
    "Regularization techniques (**L1**, **L2**, **Elastic Net**) add a penalty to large coefficients, which:\n",
    "\n",
    "* Reduces model complexity\n",
    "* Prevents overfitting\n",
    "* Improves generalization to unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e256fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf376c2a",
   "metadata": {},
   "source": [
    "## Q27. L2 regularization in Ridge Regression penalizes the : \n",
    "1. Absolute value of the coefficients \n",
    "2. Square of the coefficients \n",
    "3. Sum of the coefficients \n",
    "4. Logarithm of the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39097cde",
   "metadata": {},
   "source": [
    "The correct answer is: **2. Square of the coefficients** ✅\n",
    "\n",
    "In **Ridge Regression (L2 regularization)**, the penalty term added to the loss function is:\n",
    "\n",
    "$$\n",
    "\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "This discourages large coefficient values but doesn’t make them exactly zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6aed1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a27d322",
   "metadata": {},
   "source": [
    "## Q28. What does Lasso Regression introduce into the cost function? \n",
    "1. L1 regularization \n",
    "2. L2 regularization \n",
    "3. Elastic Net regularization \n",
    "4. No regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acbebe",
   "metadata": {},
   "source": [
    "The correct answer is: **1. L1 regularization** ✅\n",
    "\n",
    "**Lasso Regression** adds an **L1 penalty** to the cost function:\n",
    "\n",
    "$$\n",
    "\\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "This penalty can shrink some coefficients to **exactly zero**, effectively performing **feature selection**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb141a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16292bf2",
   "metadata": {},
   "source": [
    "## Q29. When would you prefer Lasso Regression over Ridge Regression? \n",
    "1. When you want to eliminate irrelevant features \n",
    "2. When all features are equally important \n",
    "3. When there is no multicollinearity \n",
    "4. When you want to increase model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b701ff",
   "metadata": {},
   "source": [
    "The correct answer is: **1. When you want to eliminate irrelevant features** ✅\n",
    "\n",
    "**Lasso Regression** (L1 regularization) can shrink some coefficients to **exactly zero**, effectively removing irrelevant features from the model — making it useful for **feature selection**.\n",
    "\n",
    "In contrast, **Ridge Regression** (L2) only shrinks coefficients but keeps all features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f994cab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397cb5a0",
   "metadata": {},
   "source": [
    "## Q30. Elastic Net is beneficial When \n",
    "1. You have a large number of correlated features \n",
    "2. You want to avoid regularization \n",
    "3. You only have a single feature \n",
    "4. You have a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6cb535",
   "metadata": {},
   "source": [
    "The correct answer is: **1. You have a large number of correlated features** ✅\n",
    "\n",
    "**Elastic Net** combines **L1** (feature selection) and **L2** (shrinkage, handles multicollinearity) penalties, making it especially useful when:\n",
    "\n",
    "* There are **many correlated predictors**\n",
    "* You want the benefits of both Lasso and Ridge methods in one model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df994b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a7290",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7439d9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4ff8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
